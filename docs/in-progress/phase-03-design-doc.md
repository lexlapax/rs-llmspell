# Phase 3: Tool Enhancement & Agent Infrastructure - Design Document

**Version**: 1.0  
**Date**: July 2025  
**Status**: Implementation Ready  
**Phase**: 3 (Tool Enhancement & Agent Infrastructure)  
**Timeline**: Weeks 9-16 (8 weeks)  
**Priority**: HIGH (MVP Completion)

> **📋 Comprehensive Implementation Guide**: This document provides complete specifications for implementing Phase 3, which spans 8 weeks and encompasses critical tool fixes, external integrations, security hardening, and agent infrastructure.

---

## Phase Overview

### Goal
Transform the existing 26 self-contained tools into a standardized, secure, and extensible library of 33+ tools, then implement comprehensive agent infrastructure and basic multi-agent coordination patterns through workflow orchestration.

### Core Principles
- **Standardization First**: Fix existing tools before adding new ones
- **Clean Break Strategy**: Direct upgrade with clear documentation (pre-1.0 freedom)
- **Security by Design**: Address all known vulnerabilities
- **Progressive Enhancement**: Each sub-phase builds on the previous
- **Documentation Driven**: Update docs before implementation

### Phase Structure
Phase 3 is divided into four 2-week sub-phases:

1. **Phase 3.0 (Weeks 9-10)**: Critical Tool Fixes - Standardization, DRY, and Initial Security
2. **Phase 3.1 (Weeks 11-12)**: External Integration Tools - 8 new tools
3. **Phase 3.2 (Weeks 13-14)**: Advanced Security & Performance - Optimization for all 33 tools
4. **Phase 3.3 (Weeks 15-16)**: Agent Infrastructure & Basic Workflows - Factory, Registry, Templates, and Multi-Agent Coordination

### Success Metrics
- **Tool Consistency**: 95% parameter standardization (from 60%)
- **DRY Compliance**: 95% shared utility usage (from 80%)
- **Security Coverage**: Comprehensive vulnerability mitigation
- **Tool Count**: 33+ production-ready tools
- **Agent Support**: Factory, Registry, and Templates operational with full tool library
- **Multi-Agent Coordination**: Basic workflow patterns (Sequential, Conditional, Loop) functional
- **Workflow-Agent Integration**: Agents can execute workflows and workflows can use agents

---

## Pre-1.0 Clean Break Benefits

### Why No Migration Tools?

1. **Development Velocity**: Save ~1 week of development time that can be invested in better features
2. **Code Cleanliness**: No legacy compatibility code cluttering the codebase
3. **Better Architecture**: Freedom to make optimal design decisions without compatibility constraints
4. **Industry Standard**: Pre-1.0 breaking changes are expected and accepted
5. **No Production Users**: Fresh from Phase 2 with minimal adoption risk

### What We Provide Instead

1. **Comprehensive Documentation**: Every breaking change documented with examples
2. **Clear Upgrade Path**: Step-by-step instructions for updating scripts
3. **Example Conversions**: Before/after code samples for common patterns
4. **Tool Reference**: Complete parameter documentation for all 33 tools

---

## Phase 3.0: Critical Tool Fixes (Weeks 9-10)

### Goal
Standardize all 26 existing tools to use consistent interfaces, parameter names, and response formats, while implementing critical security fixes.

### 1. Tool Signature Standardization

#### 1.1 Parameter Naming Convention

**Primary Data Parameter**:
```rust
// BEFORE: Inconsistent naming
- text: String         // TextManipulator
- content: String      // FileWriter  
- input: Value        // JsonProcessor
- data: String        // DataValidator
- query: String       // Calculator
- expression: String  // Calculator alternate
- template: String    // TemplateEngine

// AFTER: Standardized
- input: String | Value  // Universal primary data parameter
```

**File Path Parameters**:
```rust
// BEFORE: Inconsistent
- file_path: String
- input_path: String / output_path: String  
- path: String
- paths: Vec<String>
- archive_path: String
- file: String
- url: String

// AFTER: Standardized
- path: PathBuf              // Single file operations
- source_path: PathBuf       // Transform operations (with target_path)
- target_path: PathBuf       // Transform operations (with source_path)
- paths: Vec<PathBuf>        // Multi-file operations
```

**Operation Parameters**:
```rust
// All multi-function tools MUST use:
- operation: String  // Required for tools with multiple operations
```

#### 1.2 Detailed Parameter Mapping (26 Tools)

##### File Operations Tools (5 tools)
| Tool | Old Parameters | New Parameters | Notes |
|------|---------------|----------------|-------|
| **FileOperationsTool** | `operation: String`<br>`path: String`<br>`content: Option<String>` | `operation: String`<br>`path: PathBuf`<br>`input: Option<String>` | `content` → `input`<br>String → PathBuf |
| **FileSearchTool** | `pattern: String`<br>`path: String`<br>`search_type: String` | `operation: String`<br>`input: String`<br>`path: PathBuf`<br>`search_type: String` | Add `operation`<br>`pattern` → `input` |
| **FileConverterTool** | `input_path: String`<br>`output_path: String`<br>`format: String` | `operation: String`<br>`source_path: PathBuf`<br>`target_path: PathBuf`<br>`format: String` | Add `operation`<br>Rename paths |
| **FileWatcherTool** | `path: String`<br>`pattern: String`<br>`events: Vec<String>` | `operation: String`<br>`path: PathBuf`<br>`input: String`<br>`events: Vec<String>` | Add `operation`<br>`pattern` → `input` |
| **ArchiveHandlerTool** | `operation: String`<br>`archive_path: String`<br>`files: Vec<String>` | `operation: String`<br>`path: PathBuf`<br>`files: Vec<PathBuf>` | `archive_path` → `path` |

##### Data Processing Tools (2 tools)
| Tool | Old Parameters | New Parameters | Notes |
|------|---------------|----------------|-------|
| **CsvAnalyzerTool** | `operation: String`<br>`file_path: Option<String>`<br>`content: Option<String>` | `operation: String`<br>`input: String`<br>`path: Option<PathBuf>` | Consolidate to `input`<br>`file_path` → `path` |
| **JsonProcessorTool** | `operation: String`<br>`input: Option<Value>`<br>`query: Option<String>`<br>`content: Option<String>` | `operation: String`<br>`input: Value`<br>`query: Option<String>` | Remove duplicate `content` |

##### Media Processing Tools (3 tools)
| Tool | Old Parameters | New Parameters | Notes |
|------|---------------|----------------|-------|
| **ImageProcessorTool** | `operation: String`<br>`file_path: Option<String>`<br>`input_path: Option<String>`<br>`output_path: Option<String>` | `operation: String`<br>`source_path: PathBuf`<br>`target_path: Option<PathBuf>` | Consolidate path params |
| **AudioProcessorTool** | `operation: String`<br>`file_path: Option<String>`<br>`input_path: Option<String>`<br>`output_path: Option<String>` | `operation: String`<br>`source_path: PathBuf`<br>`target_path: Option<PathBuf>` | Consolidate path params |
| **VideoProcessorTool** | `operation: String`<br>`file_path: String`<br>`output_path: Option<String>` | `operation: String`<br>`source_path: PathBuf`<br>`target_path: Option<PathBuf>` | Standardize paths |

##### Utility Tools (9 tools)
| Tool | Old Parameters | New Parameters | Notes |
|------|---------------|----------------|-------|
| **Base64EncoderTool** | `operation: String`<br>`input: String` | `operation: String`<br>`input: String` | Already compliant ✓ |
| **CalculatorTool** | `expression: String`<br>`precision: Option<u8>` | `operation: String`<br>`input: String`<br>`precision: Option<u8>` | Add `operation`<br>`expression` → `input` |
| **TextManipulatorTool** | `text: String`<br>`operation: String` | `operation: String`<br>`input: String` | `text` → `input`<br>Reorder params |
| **HashCalculatorTool** | `algorithm: String`<br>`data: Option<String>`<br>`file_path: Option<String>` | `operation: String`<br>`input: Option<String>`<br>`path: Option<PathBuf>`<br>`algorithm: String` | Add `operation`<br>`data` → `input` |
| **DateTimeHandlerTool** | `operation: String`<br>`input: Option<String>` | `operation: String`<br>`input: Option<String>` | Already compliant ✓ |
| **UuidGeneratorTool** | `version: Option<u8>`<br>`namespace: Option<String>`<br>`name: Option<String>` | `operation: String`<br>`input: Option<String>`<br>`version: Option<u8>` | Add `operation`<br>Restructure |
| **DiffCalculatorTool** | `source: String`<br>`target: String` | `operation: String`<br>`source_path: PathBuf`<br>`target_path: PathBuf` | Add `operation`<br>Use path pattern |
| **TemplateEngineTool** | `template: String`<br>`context: Value` | `operation: String`<br>`input: String`<br>`context: Value` | Add `operation`<br>`template` → `input` |
| **DataValidationTool** | `data: Value`<br>`schema: Option<Value>` | `operation: String`<br>`input: Value`<br>`schema: Option<Value>` | Add `operation`<br>`data` → `input` |

##### System Integration Tools (4 tools)
| Tool | Old Parameters | New Parameters | Notes |
|------|---------------|----------------|-------|
| **ProcessExecutorTool** | `executable: String`<br>`arguments: Vec<String>` | `operation: String`<br>`input: String`<br>`arguments: Vec<String>` | Add `operation`<br>`executable` → `input` |
| **SystemMonitorTool** | `metrics: Vec<String>` | `operation: String`<br>`input: Vec<String>` | Add `operation`<br>`metrics` → `input` |
| **ServiceCheckerTool** | `service: String`<br>`host: String`<br>`port: u16` | `operation: String`<br>`input: String`<br>`host: String`<br>`port: u16` | Add `operation`<br>`service` → `input` |
| **EnvironmentReaderTool** | `operation: String`<br>`pattern: Option<String>` | `operation: String`<br>`input: Option<String>` | `pattern` → `input` |

##### API/Web Tools (3 tools)
| Tool | Old Parameters | New Parameters | Notes |
|------|---------------|----------------|-------|
| **HttpRequestTool** | `method: String`<br>`url: String` | `operation: String`<br>`input: String`<br>`method: String` | Add `operation`<br>`url` → `input` |
| **GraphQLQueryTool** | `endpoint: String`<br>`query: String` | `operation: String`<br>`input: String`<br>`endpoint: String` | Add `operation`<br>`query` → `input` |
| **WebSearchTool** | `query: String` | `operation: String`<br>`input: String` | Add `operation`<br>`query` → `input` |

**Summary**: 26 tools total - Only 2 already compliant (Base64EncoderTool, DateTimeHandlerTool)

#### 1.3 ResponseBuilder Pattern

**Standard Response Format**:
```rust
use llmspell_utils::response::ResponseBuilder;

#[derive(Debug, Serialize, Deserialize)]
pub struct StandardResponse {
    pub operation: String,
    pub success: bool,
    pub message: Option<String>,
    pub result: serde_json::Value,
    pub error: Option<ErrorDetails>,
}

impl ResponseBuilder {
    pub fn new(operation: &str) -> Self {
        Self {
            operation: operation.to_string(),
            success: true,
            message: None,
            result: Value::Null,
            error: None,
        }
    }
    
    pub fn with_result(mut self, result: impl Serialize) -> Self {
        self.result = serde_json::to_value(result).unwrap_or(Value::Null);
        self
    }
    
    pub fn with_error(mut self, error: &str) -> Self {
        self.success = false;
        self.error = Some(ErrorDetails::new(error));
        self
    }
    
    pub fn build(self) -> String {
        serde_json::to_string_pretty(&self).unwrap()
    }
}
```

**Validation Response Standard**:
```rust
// All validation operations MUST return:
{
    "operation": "validate",
    "success": true,
    "result": {
        "valid": bool,
        "errors": Option<Vec<ValidationError>>
    }
}
```

### 2. DRY Principle Enforcement

#### 2.1 Shared Validators (llmspell-utils)

**Move to llmspell_utils::validators**:
```rust
// From DataValidationTool -> llmspell_utils
pub fn validate_email(email: &str) -> Result<()>;
pub fn validate_url(url: &str) -> Result<()>;
pub fn validate_json_schema(data: &Value, schema: &Value) -> Result<()>;
pub fn validate_regex_pattern(pattern: &str) -> Result<()>;
pub fn validate_date_format(date: &str, format: &str) -> Result<()>;
```

#### 2.2 Shared HTTP Utilities

**Move to llmspell_utils::http**:
```rust
// From HttpRequestTool -> llmspell_utils
pub async fn retry_async<F, T>(
    operation: F,
    config: RetryConfig,
) -> Result<T>
where
    F: Fn() -> Fut,
    Fut: Future<Output = Result<T>>;

pub fn create_http_client(config: HttpClientConfig) -> Result<Client>;
pub fn parse_rate_limit_headers(headers: &HeaderMap) -> RateLimitInfo; // Now in llmspell-utils
```

#### 2.3 Shared JSON Operations

**Move to llmspell_utils::serialization**:
```rust
// Standardize all JSON operations
pub fn to_json_pretty<T: Serialize>(value: &T) -> Result<String>;
pub fn from_json_str<T: DeserializeOwned>(s: &str) -> Result<T>;
pub fn merge_json_objects(base: Value, patch: Value) -> Value;
```

### 3. Breaking Changes Documentation

#### 3.1 Pre-1.0 Clean Break Approach

**Philosophy**: As a pre-1.0 project (version 0.1.0), we're making clean breaking changes without migration tools to achieve the best possible architecture before stability commitments.

#### 3.2 Comprehensive Change Documentation

**Create CHANGELOG_v0.3.0.md**:
```markdown
# Version 0.3.0 Breaking Changes

## Parameter Standardization
| Tool | Old Parameter | New Parameter | Example |
|------|---------------|---------------|---------|
| TextManipulator | text | input | `{"input": "hello"}` |
| JsonProcessor | input/content | input | `{"input": {...}}` |
| FileOperations | file_path | path | `{"path": "/tmp/file"}` |
| Calculator | query/expression | input | `{"input": "2+2"}` |
| DataValidator | data | input | `{"input": {...}}` |
| TemplateEngine | template | input | `{"input": "Hello {{name}}"}` |

## Response Format Changes
- All tools now use ResponseBuilder pattern
- Consistent success/error structure
- Standardized field names

## Upgrade Instructions
1. Update all tool parameter names in your scripts
2. Adjust response parsing for new format
3. Review security settings for hardened defaults
```

#### 3.3 Example Conversions Guide

```rust
// Before (v0.2.0)
let params = json!({
    "text": "hello world",
    "operation": "uppercase"
});

// After (v0.3.0)
let params = json!({
    "input": "hello world",
    "operation": "uppercase"
});

// Response parsing before
let result = response["result"].as_str();

// Response parsing after
let response = serde_json::from_str::<StandardResponse>(&output)?;
let result = response.result["text"].as_str();
```

### 4. Security Hardening (Moved from Phase 3.2)

#### 4.1 Critical Security Fixes

**Calculator DoS Protection**:
```rust
// Implement in Phase 3.0 for immediate security
pub struct CalculatorLimits {
    max_expression_length: usize,  // 1000 chars
    max_complexity_score: u32,     // Prevent exponential expressions
    max_eval_time_ms: u64,         // 100ms timeout
    max_memory_bytes: usize,       // 10MB limit
}
```

**Path Security for File Tools**:
```rust
// Extract to llmspell_utils::security::paths
pub fn validate_safe_path(path: &Path, jail_dir: &Path) -> Result<()> {
    // Prevent symlink attacks
    let canonical = path.canonicalize()?;
    if !canonical.starts_with(jail_dir) {
        return Err(SecurityError::PathEscape);
    }
    // Additional checks...
}
```

### 5. Implementation Checklist

**Week 9 Tasks**:
- [ ] Create ResponseBuilder in llmspell-utils
- [ ] Extract shared validators to llmspell-utils
- [ ] Extract HTTP utilities to llmspell-utils
- [ ] Implement critical security fixes (Calculator DoS, Path security)
- [ ] Update first 12 tools to new standards
- [ ] Create comprehensive change documentation

**Week 10 Tasks**:
- [ ] Update remaining 13 tools to new standards
- [ ] Complete security hardening for all file tools
- [ ] Write example conversion guide
- [ ] Update all tool documentation
- [ ] Performance optimization for standardized tools
- [ ] Run full regression and security test suite

---

## Phase 3.1: External Integration Tools (Weeks 11-12)

### Goal
Add 8 external integration tools following the standardized patterns established in Phase 3.0.

### 1. Web & Network Tools (7 tools)

#### 1.1 WebSearchTool Enhancement

**Current State**: Basic implementation exists from Phase 2  
**Enhancement Required**: Add real search provider APIs

```rust
// llmspell-tools/src/web/web_search.rs
pub struct WebSearchTool {
    providers: HashMap<String, Box<dyn SearchProvider>>,
    default_provider: String,
    rate_limiter: RateLimiter,
}

#[async_trait]
impl Tool for WebSearchTool {
    fn schema(&self) -> ToolSchema {
        ToolSchema::new("web_search", "Enhanced web search with multiple providers")
            .with_parameter("input", ParameterType::String, "Search query", true)
            .with_parameter("provider", ParameterType::String, "google|brave|duckduckgo|serpapi|serperdev", false)
            .with_parameter("max_results", ParameterType::Integer, "1-100", false)
            .with_parameter("search_type", ParameterType::String, "web|news|images", false)
    }
    
    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let query = input.get_required_string("input")?;
        let provider = input.get_optional_string("provider")
            .unwrap_or(&self.default_provider);
        
        // Rate limiting (using llmspell-utils::ProviderRateLimiter)
        self.rate_limiter.check_and_wait(provider).await?;
        
        // Provider execution
        let results = self.providers.get(provider)
            .ok_or_else(|| ToolError::InvalidProvider(provider.to_string()))?
            .search(&query, &input).await?;
        
        // Standard response
        ResponseBuilder::new("search")
            .with_result(results)
            .build()
    }
}
```

**Search Providers**:
```rust
// DuckDuckGo (no API key required)
pub struct DuckDuckGoProvider {
    client: Client,
}

// Google Custom Search (API key required)
pub struct GoogleSearchProvider {
    api_key: String,
    search_engine_id: String,
    client: Client,
}

// Brave Search (API key required)
pub struct BraveSearchProvider {
    api_key: String,
    client: Client,
}

// SerpApi.com (API key required)
pub struct SerpApiProvider {
    api_key: String,
    client: Client,
}

// Serper.dev (API key required)
pub struct SerperDevProvider {
    api_key: String,
    client: Client,
}
```

**Rate Limiting Considerations**:
Each provider has different rate limits that must be respected:
- DuckDuckGo: No official API, web scraping limits apply
- Google Custom Search: 100 queries/day (free tier), 10,000/day (paid)
- Brave: 2,000 queries/month (free tier), higher tiers available
- SerpApi: Varies by plan (100-5,000 searches/month)
- SerperDev: 2,500 queries/month (free tier), 50k+ (paid)

**Provider Selection Strategy**:
```rust
pub struct ProviderSelector {
    providers: Vec<(String, Box<dyn SearchProvider>)>,
    rate_limiters: HashMap<String, RateLimiter>,
    priority_order: Vec<String>, // Prefer free/high-limit providers
}
```

**Configuration Management**:
With 5 providers requiring different API keys and settings:
```rust
// Environment variables for API keys
WEBSEARCH_GOOGLE_API_KEY=...
WEBSEARCH_GOOGLE_SEARCH_ENGINE_ID=...
WEBSEARCH_BRAVE_API_KEY=...
WEBSEARCH_SERPAPI_API_KEY=...
WEBSEARCH_SERPERDEV_API_KEY=...

// Configuration structure
pub struct WebSearchConfig {
    providers: HashMap<String, ProviderConfig>,
    default_provider: String,
    fallback_chain: Vec<String>, // e.g., ["duckduckgo", "serperdev", "google"]
}
```

#### 1.2 Additional Web Tools

**WebScraperTool**:
```rust
pub struct WebScraperTool {
    client: Client,
    js_renderer: Option<JsRenderer>, // For JavaScript-heavy sites
}

impl Tool for WebScraperTool {
    fn schema(&self) -> ToolSchema {
        ToolSchema::new("web_scraper", "Extract content from web pages")
            .with_parameter("input", ParameterType::String, "URL to scrape", true)
            .with_parameter("selectors", ParameterType::Object, "CSS selectors", false)
            .with_parameter("wait_for_js", ParameterType::Boolean, "Wait for JS", false)
            .with_parameter("timeout", ParameterType::Integer, "Timeout in ms", false)
    }
}
```

**Other Web Tools**:
- `UrlAnalyzerTool`: URL validation, metadata extraction
- `ApiTesterTool`: REST API testing with assertions
- `WebhookCallerTool`: Webhook invocation with retries
- `WebpageMonitorTool`: Change detection and alerting
- `SitemapCrawlerTool`: Sitemap parsing and analysis

### 2. Communication Tools (1 tool)

#### 2.1 EmailSenderTool

```rust
pub struct EmailSenderTool {
    providers: HashMap<String, Box<dyn EmailProvider>>,
    default_provider: String,
}

impl Tool for EmailSenderTool {
    fn schema(&self) -> ToolSchema {
        ToolSchema::new("email_sender", "Send emails via multiple providers")
            .with_parameter("input", ParameterType::Object, "Email content", true)
            .with_parameter("provider", ParameterType::String, "smtp|sendgrid|ses", false)
    }
}

// Email input structure
#[derive(Deserialize)]
struct EmailInput {
    to: Vec<String>,
    subject: String,
    body: String,
    html: Option<String>,
    attachments: Option<Vec<Attachment>>,
}
```

#### 2.2 DatabaseConnectorTool

```rust
pub struct DatabaseConnectorTool {
    connections: HashMap<String, DatabaseConnection>,
    query_timeout: Duration,
}

impl Tool for DatabaseConnectorTool {
    fn schema(&self) -> ToolSchema {
        ToolSchema::new("database_connector", "Execute database queries")
            .with_parameter("input", ParameterType::String, "SQL query", true)
            .with_parameter("connection", ParameterType::String, "Connection name", true)
            .with_parameter("parameters", ParameterType::Array, "Query parameters", false)
    }
}
```

**Note**: SlackIntegrationTool and GitHubIntegrationTool have been deferred to Phase 19 (Additional Optional Enhancements).

### 4. Implementation Checklist

**Week 11 Tasks**:
- [ ] Implement WebSearchTool providers (Google, Brave, DuckDuckGo, SerpApi, SerperDev)
- [x] Create provider selection and rate limiting system (completed as llmspell-utils utility)
- [x] Implement circuit breaker pattern for fault tolerance (completed as llmspell-utils utility)
- [ ] Create WebScraperTool with JS rendering
- [ ] Implement remaining web tools (5 tools)
- [ ] Create EmailSenderTool with providers
- [ ] Test all web/network tools with rate limit simulation

**Week 12 Tasks**:
- [ ] Implement DatabaseConnectorTool
- [ ] Complete EmailSenderTool testing
- [ ] Integration testing with Phase 3.0 standards
- [ ] Documentation for all 8 new tools

---

## Phase 3.2: Security & Performance (Weeks 13-14)

### Goal
Harden security across all 33 tools and optimize performance while maintaining the 52,600x speed advantage.

### 1. Security Hardening

#### 1.1 Calculator DoS Protection

**Current Vulnerability**: Calculator accepts arbitrarily complex expressions

```rust
// llmspell-tools/src/math/calculator.rs
impl CalculatorTool {
    fn validate_expression_complexity(&self, expr: &str) -> Result<()> {
        // Complexity metrics
        let depth = self.calculate_nesting_depth(expr)?;
        let operations = self.count_operations(expr)?;
        let length = expr.len();
        
        // Limits
        const MAX_DEPTH: usize = 10;
        const MAX_OPERATIONS: usize = 100;
        const MAX_LENGTH: usize = 1000;
        
        if depth > MAX_DEPTH {
            return Err(ToolError::ComplexityLimit("Nesting too deep"));
        }
        if operations > MAX_OPERATIONS {
            return Err(ToolError::ComplexityLimit("Too many operations"));
        }
        if length > MAX_LENGTH {
            return Err(ToolError::ComplexityLimit("Expression too long"));
        }
        
        Ok(())
    }
    
    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let expr = input.get_required_string("input")?;
        
        // Validate before evaluation
        self.validate_expression_complexity(&expr)?;
        
        // Timeout protection
        let result = timeout(Duration::from_secs(1), async {
            self.evaluate_expression(&expr)
        }).await??;
        
        ResponseBuilder::new("calculate")
            .with_result(result)
            .build()
    }
}
```

#### 1.2 Symlink Escape Prevention

**Current Vulnerability**: File operations may follow symlinks outside sandbox

```rust
// llmspell-security/src/sandbox/file_sandbox.rs
impl FileSandbox {
    pub fn resolve_path_safely(&self, path: &Path) -> Result<PathBuf> {
        // Canonicalize without following symlinks
        let mut resolved = self.sandbox_root.clone();
        
        for component in path.components() {
            match component {
                Component::ParentDir => {
                    if !resolved.pop() {
                        return Err(SecurityError::PathEscape);
                    }
                }
                Component::Normal(name) => {
                    resolved.push(name);
                    
                    // Check if it's a symlink
                    if resolved.is_symlink() {
                        let target = fs::read_link(&resolved)?;
                        let resolved_target = self.resolve_symlink_target(&resolved, &target)?;
                        
                        // Ensure target is within sandbox
                        if !resolved_target.starts_with(&self.sandbox_root) {
                            return Err(SecurityError::SymlinkEscape(resolved_target));
                        }
                        
                        resolved = resolved_target;
                    }
                }
                _ => {}
            }
        }
        
        // Final check
        if !resolved.starts_with(&self.sandbox_root) {
            return Err(SecurityError::PathEscape);
        }
        
        Ok(resolved)
    }
}
```

#### 1.3 Resource Limit Enforcement

```rust
// llmspell-security/src/limits.rs
#[derive(Clone, Debug)]
pub struct ResourceLimits {
    pub max_file_size: u64,         // Maximum file size for operations
    pub max_memory_usage: u64,      // Maximum memory per tool
    pub max_execution_time: Duration, // Maximum execution time
    pub max_open_files: usize,      // Maximum open file handles
    pub max_network_connections: usize, // Maximum concurrent connections
}

impl Default for ResourceLimits {
    fn default() -> Self {
        Self {
            max_file_size: 100 * 1024 * 1024,        // 100MB
            max_memory_usage: 512 * 1024 * 1024,     // 512MB
            max_execution_time: Duration::from_secs(30),
            max_open_files: 100,
            max_network_connections: 10,
        }
    }
}

// Enforcement middleware
pub struct ResourceLimitGuard {
    limits: ResourceLimits,
    current_usage: Arc<Mutex<ResourceUsage>>,
}

impl ResourceLimitGuard {
    pub async fn check_file_size(&self, size: u64) -> Result<()> {
        if size > self.limits.max_file_size {
            return Err(SecurityError::FileSizeLimit(size, self.limits.max_file_size));
        }
        Ok(())
    }
    
    pub async fn track_memory<F, R>(&self, f: F) -> Result<R>
    where
        F: FnOnce() -> R,
    {
        let start_memory = self.get_current_memory();
        let result = f();
        let end_memory = self.get_current_memory();
        
        let used = end_memory.saturating_sub(start_memory);
        if used > self.limits.max_memory_usage {
            return Err(SecurityError::MemoryLimit(used, self.limits.max_memory_usage));
        }
        
        Ok(result)
    }
}
```

### 2. Performance Optimization

#### 2.1 Shared Resource Pools

```rust
// llmspell-utils/src/pools.rs
lazy_static! {
    // Shared HTTP client pool
    static ref HTTP_CLIENT_POOL: ClientPool = ClientPool::new(
        ClientConfig {
            pool_size: 10,
            timeout: Duration::from_secs(30),
            keep_alive: true,
        }
    );
    
    // Shared regex cache
    static ref REGEX_CACHE: Mutex<LruCache<String, Regex>> = 
        Mutex::new(LruCache::new(NonZeroUsize::new(100).unwrap()));
    
    // Shared template engine
    static ref TEMPLATE_ENGINE: RwLock<Tera> = RwLock::new({
        let mut tera = Tera::default();
        tera.autoescape_on(vec![".html", ".xml"]);
        tera
    });
}

// Usage in tools
impl HttpRequestTool {
    pub fn new() -> Self {
        Self {
            client: HTTP_CLIENT_POOL.get(), // Reuse shared client
        }
    }
}
```

#### 2.2 Caching Strategies

```rust
// llmspell-utils/src/cache.rs
pub struct ToolCache {
    memory_cache: Arc<Mutex<LruCache<String, CachedResult>>>,
    disk_cache: Option<DiskCache>,
}

impl ToolCache {
    pub async fn get_or_compute<F, Fut, T>(&self, key: &str, f: F) -> Result<T>
    where
        F: FnOnce() -> Fut,
        Fut: Future<Output = Result<T>>,
        T: Serialize + DeserializeOwned,
    {
        // Check memory cache
        if let Some(cached) = self.memory_cache.lock().await.get(key) {
            if !cached.is_expired() {
                return Ok(serde_json::from_value(cached.value.clone())?);
            }
        }
        
        // Compute result
        let result = f().await?;
        
        // Cache result
        let cached = CachedResult {
            value: serde_json::to_value(&result)?,
            timestamp: Instant::now(),
            ttl: Duration::from_secs(300),
        };
        
        self.memory_cache.lock().await.put(key.to_string(), cached);
        
        Ok(result)
    }
}
```

### 3. Implementation Checklist

**Week 13 Tasks**:
- [ ] Implement calculator complexity validation
- [ ] Create symlink escape prevention
- [ ] Build resource limit framework
- [ ] Add security middleware to all tools
- [ ] Security audit first 20 tools

**Week 14 Tasks**:
- [ ] Security audit remaining 21 tools
- [ ] Implement shared resource pools
- [ ] Add caching to appropriate tools
- [ ] Performance benchmarking
- [ ] Create security test suite

---

## Phase 3.3: Agent Infrastructure (Weeks 15-16)

### Goal
Implement comprehensive agent infrastructure including factory patterns, registry system, lifecycle management, and pre-configured templates that leverage all 41+ standardized and secured tools.

**Note**: During implementation, the need for a unified storage abstraction emerged for agent registry persistence. This led to the creation of `llmspell-storage` as a foundational crate providing backend-agnostic persistence with support for memory (testing), Sled (embedded database), and future RocksDB backends. This storage layer follows the same design principles as the tool standardization efforts and provides type-safe serialization abstractions.

### 1. Agent Factory Pattern

```rust
// llmspell-agents/src/factory.rs
pub struct AgentFactory {
    builders: HashMap<String, Box<dyn AgentBuilder>>,
    tool_registry: Arc<ToolRegistry>,
    llm_providers: Arc<LlmProviderRegistry>,
    default_config: AgentConfig,
}

impl AgentFactory {
    pub fn new(tool_registry: Arc<ToolRegistry>) -> Self {
        Self {
            builders: HashMap::new(),
            tool_registry,
            llm_providers: Arc::new(LlmProviderRegistry::default()),
            default_config: AgentConfig::default(),
        }
    }
    
    pub fn register_builder<B: AgentBuilder + 'static>(&mut self, name: &str, builder: B) {
        self.builders.insert(name.to_string(), Box::new(builder));
    }
    
    pub async fn create_agent(&self, spec: &AgentSpec) -> Result<Box<dyn Agent>> {
        let builder = self.builders.get(&spec.agent_type)
            .ok_or_else(|| AgentError::UnknownType(spec.agent_type.clone()))?;
        
        let mut agent = builder.build(spec)?;
        
        // Configure with tools
        for tool_name in &spec.tools {
            let tool = self.tool_registry.get(tool_name)?;
            agent.add_tool(tool_name, tool)?;
        }
        
        // Configure LLM provider
        if let Some(provider_spec) = &spec.llm_provider {
            let provider = self.llm_providers.create(provider_spec)?;
            agent.set_llm_provider(provider)?;
        }
        
        // Apply configuration
        agent.configure(spec.config.as_ref().unwrap_or(&self.default_config))?;
        
        Ok(agent)
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentSpec {
    pub name: String,
    pub agent_type: String,
    pub description: Option<String>,
    pub tools: Vec<String>,
    pub llm_provider: Option<LlmProviderSpec>,
    pub config: Option<AgentConfig>,
    pub metadata: HashMap<String, Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentConfig {
    pub max_iterations: usize,
    pub timeout: Duration,
    pub temperature: f32,
    pub max_tokens: usize,
    pub retry_policy: RetryPolicy,
    pub memory_config: MemoryConfig,
    pub security_config: SecurityConfig,
}

pub trait AgentBuilder: Send + Sync {
    fn build(&self, spec: &AgentSpec) -> Result<Box<dyn Agent>>;
    fn supported_features(&self) -> Vec<AgentFeature>;
    fn validate_spec(&self, spec: &AgentSpec) -> Result<()>;
}
```

### 2. Agent Registry with Discovery

#### 2.1 Registry Architecture

```rust
// llmspell-agents/src/registry.rs
pub struct AgentRegistry {
    agents: Arc<RwLock<HashMap<String, AgentEntry>>>,
    factory: Arc<AgentFactory>,
    discovery: Arc<DiscoveryService>,
    persistence: Arc<dyn RegistryPersistence>,
}

#[derive(Debug, Clone)]
pub struct AgentEntry {
    pub id: String,
    pub spec: AgentSpec,
    pub instance: Option<Arc<Mutex<Box<dyn Agent>>>>,
    pub status: AgentStatus,
    pub metadata: RegistryMetadata,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RegistryMetadata {
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub last_accessed: DateTime<Utc>,
    pub access_count: u64,
    pub tags: Vec<String>,
    pub capabilities: Vec<AgentCapability>,
}

#### 2.2 Storage Backend Integration

During implementation, a unified storage abstraction layer was discovered to be essential for agent registry persistence. This led to the creation of `llmspell-storage` as a foundational crate providing backend-agnostic persistence.

##### 2.2.1 Storage Architecture

```rust
// llmspell-storage/src/traits.rs
#[async_trait]
pub trait StorageBackend: Send + Sync {
    /// Get a value by key
    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>>;
    
    /// Set a key-value pair
    async fn set(&self, key: &str, value: Vec<u8>) -> Result<()>;
    
    /// Delete a key
    async fn delete(&self, key: &str) -> Result<()>;
    
    /// Check if a key exists
    async fn exists(&self, key: &str) -> Result<bool>;
    
    /// List all keys with a given prefix
    async fn list_keys(&self, prefix: &str) -> Result<Vec<String>>;
    
    /// Get multiple values by keys (batch operation)
    async fn get_batch(&self, keys: &[String]) -> Result<HashMap<String, Vec<u8>>>;
    
    /// Set multiple key-value pairs (batch operation)
    async fn set_batch(&self, items: HashMap<String, Vec<u8>>) -> Result<()>;
    
    /// Delete multiple keys (batch operation)
    async fn delete_batch(&self, keys: &[String]) -> Result<()>;
    
    /// Clear all data (use with caution)
    async fn clear(&self) -> Result<()>;
    
    /// Get the backend type
    fn backend_type(&self) -> StorageBackendType;
    
    /// Get backend characteristics
    fn characteristics(&self) -> StorageCharacteristics;
}

/// Helper trait for serialization/deserialization
pub trait StorageSerialize: Sized {
    fn to_storage_bytes(&self) -> Result<Vec<u8>>;
    fn from_storage_bytes(bytes: &[u8]) -> Result<Self>;
}

/// Default implementation for serde types
impl<T> StorageSerialize for T
where
    T: Serialize + for<'de> Deserialize<'de>,
{
    fn to_storage_bytes(&self) -> Result<Vec<u8>> {
        Ok(serde_json::to_vec(self)?)
    }
    
    fn from_storage_bytes(bytes: &[u8]) -> Result<Self> {
        Ok(serde_json::from_slice(bytes)?)
    }
}
```

##### 2.2.2 Storage Backend Implementations

```rust
// llmspell-storage/src/backends/memory.rs
pub struct MemoryBackend {
    data: Arc<RwLock<HashMap<String, Vec<u8>>>>,
}

impl MemoryBackend {
    pub fn new() -> Self {
        Self {
            data: Arc::new(RwLock::new(HashMap::new())),
        }
    }
}

// llmspell-storage/src/backends/sled_backend.rs  
pub struct SledBackend {
    db: sled::Db,
    path: PathBuf,
}

impl SledBackend {
    pub async fn new(path: impl AsRef<Path>) -> Result<Self> {
        let path = path.as_ref().to_path_buf();
        let db = sled::open(&path)?;
        
        Ok(Self { db, path })
    }
}

// Future: RocksDB backend for high-performance scenarios
// llmspell-storage/src/backends/rocksdb_backend.rs
```

##### 2.2.3 Registry Persistence Integration

```rust
// llmspell-agents/src/registry/persistence.rs
pub struct PersistentAgentRegistry {
    /// Storage backend from llmspell-storage
    storage: Arc<dyn StorageBackend>,
    
    /// Runtime agents (not persisted)
    runtime_agents: Arc<RwLock<HashMap<String, Arc<dyn Agent>>>>,
    
    /// Metadata cache for performance
    metadata_cache: Arc<RwLock<HashMap<String, AgentMetadata>>>,
}

impl PersistentAgentRegistry {
    pub async fn new(storage: Arc<dyn StorageBackend>) -> Result<Self> {
        // Load existing metadata from storage
        let metadata = Self::load_all_metadata(&storage).await?;
        
        Ok(Self {
            storage,
            runtime_agents: Arc::new(RwLock::new(HashMap::new())),
            metadata_cache: Arc::new(RwLock::new(metadata)),
        })
    }
    
    /// Persist current state to storage
    pub async fn persist(&self) -> Result<()> {
        let cache = self.metadata_cache.read().await;
        
        // Save individual metadata entries
        for (id, metadata) in cache.iter() {
            let key = format!("agent:metadata:{}", id);
            let data = metadata.to_storage_bytes()?;
            self.storage.set(&key, data).await?;
        }
        
        // Also save a snapshot for faster loading
        let snapshot_data = cache.to_storage_bytes()?;
        self.storage.set("registry:snapshot", snapshot_data).await?;
        
        Ok(())
    }
}
```

##### 2.2.4 Storage Benefits and Design Rationale

**Key Benefits:**
- **Backend Agnostic**: Registry works with memory, disk, or future cloud storage
- **Performance Optimized**: Batch operations and caching for high throughput
- **Type Safe**: StorageSerialize trait provides compile-time serialization guarantees
- **Test Friendly**: MemoryBackend enables fast, isolated testing
- **Production Ready**: SledBackend provides ACID persistence

**Design Decisions:**
- **Trait-Based**: StorageBackend trait allows swapping implementations
- **Async First**: All operations are async for non-blocking I/O
- **Batch Operations**: Optimized for bulk operations (registry snapshots)
- **Key Namespacing**: Prefix-based key organization prevents collisions
- **Serialization Abstraction**: Generic over serde types with fallback to JSON

impl AgentRegistry {
    pub async fn new(factory: Arc<AgentFactory>, config: RegistryConfig) -> Result<Self> {
        let persistence = Self::create_persistence(&config)?;
        let discovery = Arc::new(DiscoveryService::new(config.discovery));
        
        let mut registry = Self {
            agents: Arc::new(RwLock::new(HashMap::new())),
            factory,
            discovery,
            persistence,
        };
        
        // Load persisted agents
        registry.load_from_persistence().await?;
        
        // Start discovery service
        registry.start_discovery().await?;
        
        Ok(registry)
    }
    
    pub async fn register(&self, spec: AgentSpec) -> Result<String> {
        let id = Uuid::new_v4().to_string();
        let entry = AgentEntry {
            id: id.clone(),
            spec: spec.clone(),
            instance: None,
            status: AgentStatus::Registered,
            metadata: RegistryMetadata::new(&spec),
        };
        
        self.agents.write().await.insert(id.clone(), entry);
        self.persistence.save_agent(&id, &spec).await?;
        
        Ok(id)
    }
    
    pub async fn instantiate(&self, id: &str) -> Result<Arc<Mutex<Box<dyn Agent>>>> {
        let mut agents = self.agents.write().await;
        let entry = agents.get_mut(id)
            .ok_or_else(|| RegistryError::AgentNotFound(id.to_string()))?;
        
        if let Some(instance) = &entry.instance {
            entry.metadata.last_accessed = Utc::now();
            entry.metadata.access_count += 1;
            return Ok(instance.clone());
        }
        
        // Create new instance
        let agent = self.factory.create_agent(&entry.spec).await?;
        let instance = Arc::new(Mutex::new(agent));
        
        entry.instance = Some(instance.clone());
        entry.status = AgentStatus::Active;
        entry.metadata.last_accessed = Utc::now();
        entry.metadata.access_count += 1;
        
        Ok(instance)
    }
    
    pub async fn discover(&self, query: &DiscoveryQuery) -> Result<Vec<AgentEntry>> {
        let agents = self.agents.read().await;
        let mut results = Vec::new();
        
        for entry in agents.values() {
            if self.matches_query(entry, query) {
                results.push(entry.clone());
            }
        }
        
        // Sort by relevance
        results.sort_by(|a, b| {
            self.calculate_relevance(a, query)
                .partial_cmp(&self.calculate_relevance(b, query))
                .unwrap_or(Ordering::Equal)
                .reverse()
        });
        
        Ok(results)
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiscoveryQuery {
    pub capabilities: Vec<AgentCapability>,
    pub tags: Vec<String>,
    pub tools: Vec<String>,
    pub text_search: Option<String>,
    pub filters: HashMap<String, Value>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum AgentCapability {
    Research,
    CodeGeneration,
    DataAnalysis,
    Conversation,
    TaskAutomation,
    ApiIntegration,
    FileProcessing,
    WebScraping,
    Custom(String),
}
```

#### 2.2 Discovery Service

```rust
pub struct DiscoveryService {
    providers: Vec<Box<dyn DiscoveryProvider>>,
    cache: Arc<Mutex<DiscoveryCache>>,
    config: DiscoveryConfig,
}

#[async_trait]
pub trait DiscoveryProvider: Send + Sync {
    async fn discover(&self, query: &DiscoveryQuery) -> Result<Vec<DiscoveredAgent>>;
    fn provider_name(&self) -> &str;
    fn supports_capability(&self, capability: &AgentCapability) -> bool;
}

// Local file-based discovery
pub struct FileDiscoveryProvider {
    search_paths: Vec<PathBuf>,
    file_patterns: Vec<String>,
}

#[async_trait]
impl DiscoveryProvider for FileDiscoveryProvider {
    async fn discover(&self, query: &DiscoveryQuery) -> Result<Vec<DiscoveredAgent>> {
        let mut results = Vec::new();
        
        for path in &self.search_paths {
            for entry in WalkDir::new(path) {
                if let Ok(entry) = entry {
                    if self.matches_pattern(&entry) {
                        if let Ok(spec) = self.load_agent_spec(&entry.path()).await {
                            if self.matches_query(&spec, query) {
                                results.push(DiscoveredAgent {
                                    spec,
                                    source: DiscoverySource::File(entry.path().to_path_buf()),
                                    confidence: 1.0,
                                });
                            }
                        }
                    }
                }
            }
        }
        
        Ok(results)
    }
}

// Network-based discovery
pub struct NetworkDiscoveryProvider {
    registry_urls: Vec<String>,
    client: reqwest::Client,
}

#[async_trait]
impl DiscoveryProvider for NetworkDiscoveryProvider {
    async fn discover(&self, query: &DiscoveryQuery) -> Result<Vec<DiscoveredAgent>> {
        let mut results = Vec::new();
        
        for url in &self.registry_urls {
            let response = self.client
                .post(format!("{}/discover", url))
                .json(query)
                .send()
                .await?;
                
            let agents: Vec<DiscoveredAgent> = response.json().await?;
            results.extend(agents);
        }
        
        Ok(results)
    }
}
```

### 3. Agent Lifecycle Management

```rust
// llmspell-agents/src/lifecycle.rs
pub struct AgentLifecycleManager {
    registry: Arc<AgentRegistry>,
    state_store: Arc<dyn StateStore>,
    monitor: Arc<AgentMonitor>,
    scheduler: Arc<AgentScheduler>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum AgentState {
    Created,
    Initializing,
    Ready,
    Running,
    Paused,
    Stopping,
    Stopped,
    Failed(AgentError),
}

impl AgentLifecycleManager {
    pub async fn transition(&self, agent_id: &str, new_state: AgentState) -> Result<()> {
        let current_state = self.state_store.get_state(agent_id).await?;
        
        // Validate transition
        if !self.is_valid_transition(&current_state, &new_state) {
            return Err(LifecycleError::InvalidTransition(
                current_state,
                new_state
            ));
        }
        
        // Execute transition
        match new_state {
            AgentState::Initializing => self.initialize_agent(agent_id).await?,
            AgentState::Running => self.start_agent(agent_id).await?,
            AgentState::Paused => self.pause_agent(agent_id).await?,
            AgentState::Stopping => self.stop_agent(agent_id).await?,
            _ => {}
        }
        
        // Update state
        self.state_store.set_state(agent_id, new_state).await?;
        
        // Notify monitors
        self.monitor.notify_state_change(agent_id, current_state, new_state).await;
        
        Ok(())
    }
    
    fn is_valid_transition(&self, from: &AgentState, to: &AgentState) -> bool {
        match (from, to) {
            (AgentState::Created, AgentState::Initializing) => true,
            (AgentState::Initializing, AgentState::Ready) => true,
            (AgentState::Ready, AgentState::Running) => true,
            (AgentState::Running, AgentState::Paused) => true,
            (AgentState::Running, AgentState::Stopping) => true,
            (AgentState::Paused, AgentState::Running) => true,
            (AgentState::Paused, AgentState::Stopping) => true,
            (AgentState::Stopping, AgentState::Stopped) => true,
            (_, AgentState::Failed(_)) => true,
            _ => false,
        }
    }
}

// State machine for agent lifecycle
pub struct AgentStateMachine {
    states: HashMap<AgentState, StateHandler>,
    transitions: HashMap<(AgentState, AgentState), TransitionHandler>,
}

type StateHandler = Box<dyn Fn(&Agent) -> Result<()> + Send + Sync>;
type TransitionHandler = Box<dyn Fn(&Agent, &AgentState, &AgentState) -> Result<()> + Send + Sync>;

impl AgentStateMachine {
    pub fn new() -> Self {
        let mut sm = Self {
            states: HashMap::new(),
            transitions: HashMap::new(),
        };
        
        // Register state handlers
        sm.register_state(AgentState::Initializing, Box::new(|agent| {
            agent.load_tools()?;
            agent.connect_llm()?;
            agent.initialize_memory()?;
            Ok(())
        }));
        
        sm.register_state(AgentState::Running, Box::new(|agent| {
            agent.start_message_loop()?;
            agent.enable_monitoring()?;
            Ok(())
        }));
        
        sm.register_state(AgentState::Stopping, Box::new(|agent| {
            agent.stop_message_loop()?;
            agent.save_state()?;
            agent.cleanup_resources()?;
            Ok(())
        }));
        
        sm
    }
}
```

### 4. Agent Templates

```rust
// llmspell-agents/src/templates/mod.rs
pub trait AgentTemplate {
    fn name(&self) -> &str;
    fn description(&self) -> &str;
    fn build_spec(&self, params: &TemplateParams) -> Result<AgentSpec>;
    fn default_tools(&self) -> Vec<&'static str>;
    fn required_capabilities(&self) -> Vec<AgentCapability>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TemplateParams {
    pub name: String,
    pub custom_tools: Vec<String>,
    pub llm_config: Option<LlmConfig>,
    pub memory_size: Option<usize>,
    pub custom_prompts: HashMap<String, String>,
}

// Research Agent Template
pub struct ResearchAgentTemplate;

impl AgentTemplate for ResearchAgentTemplate {
    fn name(&self) -> &str { "research_agent" }
    
    fn description(&self) -> &str {
        "Agent optimized for research tasks with web search, content analysis, and summarization"
    }
    
    fn default_tools(&self) -> Vec<&'static str> {
        vec![
            "web_search",
            "web_scraper",
            "text_summarizer",
            "file_writer",
            "json_processor",
            "template_engine",
        ]
    }
    
    fn build_spec(&self, params: &TemplateParams) -> Result<AgentSpec> {
        let mut tools = self.default_tools().iter().map(|s| s.to_string()).collect::<Vec<_>>();
        tools.extend(params.custom_tools.clone());
        
        Ok(AgentSpec {
            name: params.name.clone(),
            agent_type: "research".to_string(),
            description: Some(self.description().to_string()),
            tools,
            llm_provider: Some(LlmProviderSpec {
                provider: "openai".to_string(),
                model: "gpt-4".to_string(),
                config: params.llm_config.clone(),
            }),
            config: Some(AgentConfig {
                max_iterations: 20,
                timeout: Duration::from_secs(300),
                temperature: 0.7,
                max_tokens: 2000,
                retry_policy: RetryPolicy::exponential(3),
                memory_config: MemoryConfig::conversational(10),
                security_config: SecurityConfig::standard(),
            }),
            metadata: hashmap!{
                "template".to_string() => json!(self.name()),
                "version".to_string() => json!("1.0"),
            },
        })
    }
}

// Code Generation Agent Template
pub struct CodeAgentTemplate;

impl AgentTemplate for CodeAgentTemplate {
    fn name(&self) -> &str { "code_agent" }
    
    fn default_tools(&self) -> Vec<&'static str> {
        vec![
            "file_operations",
            "file_search",
            "process_executor",
            "git_operations",
            "code_analyzer",
            "test_runner",
            "linter",
        ]
    }
    
    fn build_spec(&self, params: &TemplateParams) -> Result<AgentSpec> {
        // Implementation similar to research agent
        // but with code-specific configuration
        Ok(AgentSpec {
            name: params.name.clone(),
            agent_type: "code".to_string(),
            tools: self.default_tools().iter().map(|s| s.to_string()).collect(),
            config: Some(AgentConfig {
                temperature: 0.2, // Lower temperature for code
                security_config: SecurityConfig::restricted(), // Sandbox file access
                ..Default::default()
            }),
            ..Default::default()
        })
    }
}
```

#### 4.1 Additional Agent Templates

Pre-configured templates for common agent patterns.

```rust
// Chat Agent Template
pub struct ChatAgentTemplate;

impl AgentTemplate for ChatAgentTemplate {
    fn name(&self) -> &str { "chat_agent" }
    
    fn default_tools(&self) -> Vec<&'static str> {
        vec![
            "text_manipulator",
            "template_engine",
            "datetime_handler",
            "calculator",
        ]
    }
    
    fn build_spec(&self, params: &TemplateParams) -> Result<AgentSpec> {
        Ok(AgentSpec {
            name: params.name.clone(),
            agent_type: "chat".to_string(),
            config: Some(AgentConfig {
                temperature: 0.9,
                memory_config: MemoryConfig::conversational(50),
                ..Default::default()
            }),
            ..Default::default()
        })
    }
}

// API Integration Agent Template
pub struct ApiAgentTemplate;

impl AgentTemplate for ApiAgentTemplate {
    fn name(&self) -> &str { "api_agent" }
    
    fn default_tools(&self) -> Vec<&'static str> {
        vec![
            "http_request",
            "graphql_query",
            "json_processor",
            "data_validator",
            "webhook_caller",
            "api_tester",
        ]
    }
}

// Data Processing Agent Template
pub struct DataAgentTemplate;

impl AgentTemplate for DataAgentTemplate {
    fn name(&self) -> &str { "data_agent" }
    
    fn default_tools(&self) -> Vec<&'static str> {
        vec![
            "csv_analyzer",
            "json_processor",
            "database_connector",
            "data_validator",
            "file_converter",
            "archive_handler",
        ]
    }
}
```

### 5. Enhanced ExecutionContext

```rust
// llmspell-agents/src/context.rs
pub struct ExecutionContext {
    pub agent_id: String,
    pub session_id: String,
    pub services: ServiceBundle,
    pub state: Arc<RwLock<ContextState>>,
    pub metrics: Arc<Mutex<ContextMetrics>>,
}

#[derive(Clone)]
pub struct ServiceBundle {
    pub tool_registry: Arc<ToolRegistry>,
    pub llm_provider: Arc<dyn LlmProvider>,
    pub memory_store: Arc<dyn MemoryStore>,
    pub event_bus: Arc<EventBus>,
    pub logger: Arc<Logger>,
    pub cache: Arc<dyn CacheProvider>,
}

impl ExecutionContext {
    pub fn new(agent_id: String, services: ServiceBundle) -> Self {
        Self {
            agent_id: agent_id.clone(),
            session_id: Uuid::new_v4().to_string(),
            services,
            state: Arc::new(RwLock::new(ContextState::new())),
            metrics: Arc::new(Mutex::new(ContextMetrics::new())),
        }
    }
    
    pub async fn with_tool<F, R>(&self, tool_name: &str, f: F) -> Result<R>
    where
        F: FnOnce(&dyn Tool) -> Fut,
        Fut: Future<Output = Result<R>>,
    {
        let tool = self.services.tool_registry.get(tool_name)?;
        self.metrics.lock().await.record_tool_use(tool_name);
        f(tool.as_ref()).await
    }
    
    pub async fn store_memory(&self, key: &str, value: Value) -> Result<()> {
        self.services.memory_store.set(key, value).await
    }
    
    pub async fn recall_memory(&self, key: &str) -> Result<Option<Value>> {
        self.services.memory_store.get(key).await
    }
}
```

### 6. BaseAgent Tool Integration Infrastructure (Clean Trait Separation)

**Architecture Decision**: Use a separate `ToolCapable` trait to avoid polluting the foundational `BaseAgent` trait with specialized tool functionality. This ensures clean separation of concerns and prevents trait cyclicity issues.

```rust
// llmspell-core/src/traits/base_agent.rs - Foundation trait remains clean
pub trait BaseAgent: Send + Sync {
    fn metadata(&self) -> &ComponentMetadata;
    async fn execute(&self, input: AgentInput, context: ExecutionContext) -> Result<AgentOutput>;
    async fn validate_input(&self, input: &AgentInput) -> Result<()>;
    async fn handle_error(&self, error: LLMSpellError) -> Result<AgentOutput>;
    // ... other core methods only
}

// llmspell-core/src/traits/tool_capable.rs - Separate trait for tool integration
pub trait ToolCapable: BaseAgent {
    /// Discover available tools based on query criteria
    async fn discover_tools(&self, query: &ToolQuery) -> Result<Vec<ToolInfo>>;
    
    /// Invoke a tool by name with given parameters
    async fn invoke_tool(&self, tool_name: &str, parameters: JsonValue, context: ExecutionContext) -> Result<AgentOutput>;
    
    /// List all available tools that this component can access
    async fn list_available_tools(&self) -> Result<Vec<String>>;
    
    /// Check if a specific tool is available for invocation
    async fn tool_available(&self, tool_name: &str) -> bool;
    
    /// Get information about a specific tool
    async fn get_tool_info(&self, tool_name: &str) -> Result<Option<ToolInfo>>;
    
    /// Compose multiple tools into a workflow
    async fn compose_tools(&self, composition: &ToolComposition, context: ExecutionContext) -> Result<AgentOutput>;
}

// Supporting types for tool integration
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct ToolQuery {
    pub categories: Vec<String>,
    pub capabilities: Vec<String>,
    pub max_security_level: Option<String>,
    pub text_search: Option<String>,
    pub custom_filters: HashMap<String, JsonValue>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ToolInfo {
    pub name: String,
    pub description: String,
    pub category: String,
    pub security_level: String,
    pub schema: JsonValue,
    pub capabilities: Vec<String>,
    pub requirements: JsonValue,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ToolComposition {
    pub name: String,
    pub description: String,
    pub steps: Vec<ToolCompositionStep>,
    pub parallel: bool,
}
```

**Benefits of Trait Separation**:
- ✅ **Clean Foundation**: `BaseAgent` remains focused on core functionality
- ✅ **Optional Capability**: Components opt-in to tool integration via `ToolCapable`
- ✅ **No Cyclicity**: `Tool: BaseAgent` and `ToolCapable: BaseAgent` with no circular dependencies
- ✅ **Composable**: Components can implement just `BaseAgent` or both traits as needed

// Tool manager for BaseAgent implementations
pub struct ToolManager {
    tools: HashMap<String, Box<dyn Tool>>,
    tool_metadata: HashMap<String, ToolMetadata>,
    execution_context: ExecutionContext,
}

impl ToolManager {
    pub fn new(execution_context: ExecutionContext) -> Self {
        Self {
            tools: HashMap::new(),
            tool_metadata: HashMap::new(),
            execution_context,
        }
    }
    
    pub async fn register_tool(&mut self, name: &str, tool: Box<dyn Tool>) -> Result<()> {
        // Validate tool compatibility
        self.validate_tool_compatibility(&tool)?;
        
        // Extract tool metadata
        let metadata = ToolMetadata {
            name: name.to_string(),
            description: tool.description().to_string(),
            parameters: tool.parameters().clone(),
            capabilities: tool.capabilities().clone(),
        };
        
        self.tools.insert(name.to_string(), tool);
        self.tool_metadata.insert(name.to_string(), metadata);
        Ok(())
    }
    
    pub async fn invoke_with_validation(
        &self, 
        name: &str, 
        input: ToolInput,
        context: &ExecutionContext
    ) -> Result<ToolOutput> {
        // Parameter validation
        let tool = self.tools.get(name)
            .ok_or_else(|| LLMSpellError::ToolNotFound(name.to_string()))?;
        
        // Validate input against tool schema
        self.validate_tool_input(tool, &input)?;
        
        // Execute with context propagation
        let enhanced_input = self.enhance_input_with_context(input, context)?;
        tool.execute(enhanced_input).await
    }
    
    pub fn available_tools(&self) -> Vec<&str> {
        self.tools.keys().map(|k| k.as_str()).collect()
    }
    
    pub fn has_tool(&self, name: &str) -> bool {
        self.tools.contains_key(name)
    }
    
    async fn validate_tool_compatibility(&self, tool: &Box<dyn Tool>) -> Result<()> {
        // Check version compatibility
        // Check security requirements
        // Check resource requirements
        Ok(())
    }
    
    fn validate_tool_input(&self, tool: &Box<dyn Tool>, input: &ToolInput) -> Result<()> {
        // Validate against tool schema
        // Type checking
        // Parameter validation
        Ok(())
    }
    
    fn enhance_input_with_context(&self, input: ToolInput, context: &ExecutionContext) -> Result<ToolInput> {
        // Add execution context to tool input
        // Propagate security context
        // Add resource limits
        Ok(input)
    }
}

// Agent-as-tool wrapping support
pub struct AgentWrappedTool {
    agent: Box<dyn Agent>,
    tool_interface: ToolInterface,
    recursion_detector: RecursionDetector,
}

impl AgentWrappedTool {
    pub fn new(agent: Box<dyn Agent>, tool_interface: ToolInterface) -> Self {
        Self {
            agent,
            tool_interface,
            recursion_detector: RecursionDetector::new(),
        }
    }
}

impl Tool for AgentWrappedTool {
    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        // Prevent infinite recursion
        self.recursion_detector.check_recursion(&input)?;
        
        // Convert tool input to agent input
        let agent_input = self.tool_interface.convert_input(input)?;
        
        // Execute agent
        let agent_output = self.agent.process(agent_input).await?;
        
        // Convert agent output back to tool output
        self.tool_interface.convert_output(agent_output)
    }
    
    fn name(&self) -> &str {
        &self.tool_interface.name
    }
    
    fn description(&self) -> &str {
        &self.tool_interface.description
    }
    
    fn parameters(&self) -> &ToolParameters {
        &self.tool_interface.parameters
    }
}

// Tool interface for agent-as-tool conversion
pub struct ToolInterface {
    name: String,
    description: String,
    parameters: ToolParameters,
    input_converter: Box<dyn InputConverter>,
    output_converter: Box<dyn OutputConverter>,
}

impl ToolInterface {
    pub fn convert_input(&self, input: ToolInput) -> Result<AgentInput> {
        self.input_converter.convert(input)
    }
    
    pub fn convert_output(&self, output: AgentOutput) -> Result<ToolOutput> {
        self.output_converter.convert(output)
    }
}

// Recursion detection for agent-as-tool
pub struct RecursionDetector {
    call_stack: Vec<String>,
    max_depth: usize,
}

impl RecursionDetector {
    pub fn new() -> Self {
        Self {
            call_stack: Vec::new(),
            max_depth: 10,
        }
    }
    
    pub fn check_recursion(&self, input: &ToolInput) -> Result<()> {
        if self.call_stack.len() >= self.max_depth {
            return Err(LLMSpellError::RecursionLimit("Maximum recursion depth exceeded".to_string()));
        }
        
        // Check for circular dependencies
        let input_signature = self.create_input_signature(input);
        if self.call_stack.contains(&input_signature) {
            return Err(LLMSpellError::CircularDependency("Circular tool dependency detected".to_string()));
        }
        
        Ok(())
    }
    
    fn create_input_signature(&self, input: &ToolInput) -> String {
        // Create a unique signature for the input to detect recursion
        format!("{:?}", input)
    }
}

// Tool composition patterns
pub trait ToolComposition {
    fn compose_tools(&self, tools: Vec<&str>) -> Result<ComposedTool>;
}

pub struct ComposedTool {
    tools: Vec<Box<dyn Tool>>,
    composition_pattern: CompositionPattern,
}

pub enum CompositionPattern {
    Sequential,  // Tools execute in sequence
    Parallel,    // Tools execute in parallel
    Conditional, // Tools execute based on conditions
    Pipeline,    // Output of one tool feeds into next
}

impl Tool for ComposedTool {
    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        match self.composition_pattern {
            CompositionPattern::Sequential => self.execute_sequential(input).await,
            CompositionPattern::Parallel => self.execute_parallel(input).await,
            CompositionPattern::Conditional => self.execute_conditional(input).await,
            CompositionPattern::Pipeline => self.execute_pipeline(input).await,
        }
    }
    
    fn name(&self) -> &str { "composed_tool" }
    fn description(&self) -> &str { "A composition of multiple tools" }
    fn parameters(&self) -> &ToolParameters { &ToolParameters::default() }
}

impl ComposedTool {
    async fn execute_sequential(&self, input: ToolInput) -> Result<ToolOutput> {
        let mut results = Vec::new();
        for tool in &self.tools {
            let result = tool.execute(input.clone()).await?;
            results.push(result);
        }
        Ok(ToolOutput::Multiple(results))
    }
    
    async fn execute_parallel(&self, input: ToolInput) -> Result<ToolOutput> {
        let tasks: Vec<_> = self.tools.iter()
            .map(|tool| tool.execute(input.clone()))
            .collect();
        
        let results = futures::future::join_all(tasks).await;
        let successful_results: Result<Vec<_>, _> = results.into_iter().collect();
        Ok(ToolOutput::Multiple(successful_results?))
    }
    
    async fn execute_conditional(&self, input: ToolInput) -> Result<ToolOutput> {
        // Implement conditional logic based on input
        // For now, just execute the first tool
        if let Some(tool) = self.tools.first() {
            tool.execute(input).await
        } else {
            Err(LLMSpellError::Tool("No tools available for conditional execution".to_string()))
        }
    }
    
    async fn execute_pipeline(&self, input: ToolInput) -> Result<ToolOutput> {
        let mut current_input = input;
        
        for tool in &self.tools {
            let output = tool.execute(current_input).await?;
            current_input = self.convert_output_to_input(output)?;
        }
        
        Ok(ToolOutput::from_input(current_input))
    }
    
    fn convert_output_to_input(&self, output: ToolOutput) -> Result<ToolInput> {
        // Convert tool output back to input for next tool in pipeline
        // This is a simplified implementation
        Ok(ToolInput::from_output(output))
    }
}
```

### 7. Script-to-Agent Integration

```rust
// llmspell-bridge/src/agent_bridge.rs
pub struct AgentBridge {
    agent_registry: Arc<AgentRegistry>,
    script_engine: Arc<dyn ScriptEngineBridge>,
    parameter_converter: ParameterConverter,
    result_handler: ResultHandler,
}

impl AgentBridge {
    pub fn new(
        agent_registry: Arc<AgentRegistry>,
        script_engine: Arc<dyn ScriptEngineBridge>
    ) -> Self {
        Self {
            agent_registry,
            script_engine,
            parameter_converter: ParameterConverter::new(),
            result_handler: ResultHandler::new(),
        }
    }
    
    pub async fn register_agents_with_script(&self) -> Result<()> {
        let agents = self.agent_registry.list_agents().await?;
        
        for agent in agents {
            let script_callable = self.create_script_callable(agent)?;
            self.script_engine.register_function(
                &agent.name(),
                script_callable
            ).await?;
        }
        Ok(())
    }
    
    pub async fn call_agent_from_script(
        &self,
        agent_name: &str,
        script_params: ScriptValue
    ) -> Result<ScriptValue> {
        // Convert script parameters to agent input
        let agent_input = self.parameter_converter.script_to_agent(script_params)?;
        
        // Get agent from registry
        let agent = self.agent_registry.get(agent_name).await?;
        
        // Execute agent
        let agent_output = agent.process(agent_input).await?;
        
        // Convert agent output back to script value
        self.result_handler.agent_to_script(agent_output)
    }
    
    fn create_script_callable(&self, agent: &dyn Agent) -> Result<ScriptCallable> {
        let agent_name = agent.name().to_string();
        let bridge = Arc::clone(&self);
        
        Ok(ScriptCallable::new(move |params: ScriptValue| {
            let bridge = Arc::clone(&bridge);
            let agent_name = agent_name.clone();
            
            Box::pin(async move {
                bridge.call_agent_from_script(&agent_name, params).await
            })
        }))
    }
}

// Parameter conversion between script types and agent inputs
pub struct ParameterConverter {
    type_mapping: HashMap<String, AgentInputType>,
}

impl ParameterConverter {
    pub fn new() -> Self {
        Self {
            type_mapping: Self::create_default_type_mapping(),
        }
    }
    
    pub fn script_to_agent(&self, script_value: ScriptValue) -> Result<AgentInput> {
        match script_value {
            ScriptValue::String(s) => Ok(AgentInput::text(s)),
            ScriptValue::Table(t) => self.convert_table_to_input(t),
            ScriptValue::Array(a) => self.convert_array_to_input(a),
            ScriptValue::Number(n) => Ok(AgentInput::text(n.to_string())),
            ScriptValue::Boolean(b) => Ok(AgentInput::text(b.to_string())),
            ScriptValue::Nil => Ok(AgentInput::default()),
            _ => Err(LLMSpellError::InvalidParameter("Unsupported script type".to_string())),
        }
    }
    
    fn convert_table_to_input(&self, table: ScriptTable) -> Result<AgentInput> {
        let mut input = AgentInput::default();
        
        for (key, value) in table {
            match key.as_str() {
                "message" => input.message = value.as_string()?,
                "context" => input.context = self.convert_to_context(value)?,
                "metadata" => input.metadata = self.convert_to_metadata(value)?,
                "session_id" => input.session_id = Some(value.as_string()?),
                _ => {
                    // Store unknown keys in metadata
                    input.metadata.insert(key, value.into());
                }
            }
        }
        
        Ok(input)
    }
    
    fn convert_array_to_input(&self, array: Vec<ScriptValue>) -> Result<AgentInput> {
        // Convert array to a multi-part message
        let messages: Result<Vec<String>, _> = array.into_iter()
            .map(|v| v.as_string())
            .collect();
        
        Ok(AgentInput::text(messages?.join("\n")))
    }
    
    fn convert_to_context(&self, value: ScriptValue) -> Result<AgentContext> {
        let mut context = AgentContext::default();
        
        if let ScriptValue::Table(table) = value {
            for (key, value) in table {
                context.insert(key, value.into());
            }
        }
        
        Ok(context)
    }
    
    fn convert_to_metadata(&self, value: ScriptValue) -> Result<HashMap<String, serde_json::Value>> {
        let mut metadata = HashMap::new();
        
        if let ScriptValue::Table(table) = value {
            for (key, value) in table {
                metadata.insert(key, value.into());
            }
        }
        
        Ok(metadata)
    }
    
    fn create_default_type_mapping() -> HashMap<String, AgentInputType> {
        let mut mapping = HashMap::new();
        mapping.insert("text".to_string(), AgentInputType::Text);
        mapping.insert("json".to_string(), AgentInputType::Json);
        mapping.insert("binary".to_string(), AgentInputType::Binary);
        mapping
    }
}

// Result handling from agents back to scripts
pub struct ResultHandler {
    format_config: ResultFormatConfig,
}

impl ResultHandler {
    pub fn new() -> Self {
        Self {
            format_config: ResultFormatConfig::default(),
        }
    }
    
    pub fn agent_to_script(&self, agent_output: AgentOutput) -> Result<ScriptValue> {
        let mut result = ScriptTable::new();
        
        result.insert("content".to_string(), ScriptValue::String(agent_output.content));
        result.insert("success".to_string(), ScriptValue::Boolean(agent_output.success));
        
        if let Some(metadata) = agent_output.metadata {
            result.insert("metadata".to_string(), self.convert_metadata_to_script(metadata)?);
        }
        
        if let Some(error) = agent_output.error {
            result.insert("error".to_string(), ScriptValue::String(error.to_string()));
        }
        
        if let Some(session_id) = agent_output.session_id {
            result.insert("session_id".to_string(), ScriptValue::String(session_id));
        }
        
        Ok(ScriptValue::Table(result))
    }
    
    fn convert_metadata_to_script(&self, metadata: HashMap<String, serde_json::Value>) -> Result<ScriptValue> {
        let mut script_table = ScriptTable::new();
        
        for (key, value) in metadata {
            let script_value = self.json_to_script_value(value)?;
            script_table.insert(key, script_value);
        }
        
        Ok(ScriptValue::Table(script_table))
    }
    
    fn json_to_script_value(&self, value: serde_json::Value) -> Result<ScriptValue> {
        match value {
            serde_json::Value::String(s) => Ok(ScriptValue::String(s)),
            serde_json::Value::Number(n) => {
                if let Some(i) = n.as_i64() {
                    Ok(ScriptValue::Integer(i))
                } else if let Some(f) = n.as_f64() {
                    Ok(ScriptValue::Number(f))
                } else {
                    Ok(ScriptValue::String(n.to_string()))
                }
            },
            serde_json::Value::Bool(b) => Ok(ScriptValue::Boolean(b)),
            serde_json::Value::Array(arr) => {
                let script_array: Result<Vec<ScriptValue>, _> = arr.into_iter()
                    .map(|v| self.json_to_script_value(v))
                    .collect();
                Ok(ScriptValue::Array(script_array?))
            },
            serde_json::Value::Object(obj) => {
                let mut script_table = ScriptTable::new();
                for (key, value) in obj {
                    script_table.insert(key, self.json_to_script_value(value)?);
                }
                Ok(ScriptValue::Table(script_table))
            },
            serde_json::Value::Null => Ok(ScriptValue::Nil),
        }
    }
}

// Agent discovery from scripts
pub trait AgentDiscovery {
    async fn discover_agents(&self) -> Result<Vec<AgentMetadata>>;
    async fn get_agent_info(&self, name: &str) -> Result<AgentInfo>;
    async fn list_agent_templates(&self) -> Result<Vec<AgentTemplateInfo>>;
}

impl AgentDiscovery for AgentBridge {
    async fn discover_agents(&self) -> Result<Vec<AgentMetadata>> {
        self.agent_registry.list_available_agents().await
    }
    
    async fn get_agent_info(&self, name: &str) -> Result<AgentInfo> {
        let agent = self.agent_registry.get(name).await?;
        
        Ok(AgentInfo {
            name: agent.name().to_string(),
            description: agent.description().to_string(),
            parameters: agent.expected_parameters(),
            capabilities: agent.capabilities(),
            examples: agent.usage_examples(),
            tools: agent.available_tools(),
            status: agent.status(),
        })
    }
    
    async fn list_agent_templates(&self) -> Result<Vec<AgentTemplateInfo>> {
        // Get available templates from agent factory
        let factory = self.agent_registry.get_factory().await?;
        factory.list_templates().await
    }
}

// Configuration for result formatting
pub struct ResultFormatConfig {
    pub include_metadata: bool,
    pub include_timing: bool,
    pub include_debug_info: bool,
    pub max_content_length: Option<usize>,
}

impl Default for ResultFormatConfig {
    fn default() -> Self {
        Self {
            include_metadata: true,
            include_timing: false,
            include_debug_info: false,
            max_content_length: Some(10_000),
        }
    }
}

// Script callable wrapper for async agent functions
pub struct ScriptCallable {
    function: Box<dyn Fn(ScriptValue) -> Pin<Box<dyn Future<Output = Result<ScriptValue>> + Send>> + Send + Sync>,
}

impl ScriptCallable {
    pub fn new<F, Fut>(f: F) -> Self 
    where
        F: Fn(ScriptValue) -> Fut + Send + Sync + 'static,
        Fut: Future<Output = Result<ScriptValue>> + Send + 'static,
    {
        Self {
            function: Box::new(move |params| Box::pin(f(params))),
        }
    }
    
    pub async fn call(&self, params: ScriptValue) -> Result<ScriptValue> {
        (self.function)(params).await
    }
}
```

### 7.1 Current Implementation Status

**Implemented**:
- ✅ Basic AgentBridge structure with agent management
- ✅ Lua API for agent creation and discovery
- ✅ Simple parameter conversion (Lua tables to AgentInput)
- ✅ Agent registry integration
- ✅ Basic agent types (BasicAgent, SimpleProviderAgent)

**Not Yet Implemented**:
- ❌ Full bidirectional parameter conversion with complex types
- ❌ Agent-to-tool invocation through the bridge
- ❌ ScriptCallable pattern for async operations
- ❌ Comprehensive error handling and result transformation
- ❌ Performance optimization for bridge operations
- ❌ Integration with workflow patterns
- ❌ Monitoring and observability access
- ❌ Lifecycle management beyond create/delete
- ❌ Enhanced ExecutionContext support
- ❌ Composition patterns access

### 7.2 Bridge Implementation Gaps

The current implementation only exposes ~20% of the Phase 3.3 agent infrastructure:

1. **Tool Integration Gap**: Agents created via bridge cannot discover or invoke the 33+ tools
2. **Monitoring Gap**: No access to metrics, events, alerts, or performance tracking
3. **Lifecycle Gap**: Only basic create/remove, no state machine or event hooks
4. **Context Gap**: Only default ExecutionContext, no hierarchical or shared contexts
5. **Composition Gap**: No access to hierarchical, delegation, or pipeline patterns
6. **Workflow Gap**: No workflow bridge exists at all

### 7.3 Remaining Work for Complete Bridge

1. **Tool Discovery from Agents via Bridge** (Critical):
   - Extend AgentBridge to expose tool registry to agents
   - Add Lua API methods for tool discovery from within agent context
   - Implement tool invocation wrapper for script-created agents
   - Handle parameter conversion for tool inputs/outputs

2. **Bidirectional Communication** (High):
   - Complete the ParameterConverter for all AgentInput/Output types
   - Add streaming support for progressive results
   - Implement callback mechanisms for long-running operations
   - Support multimodal content (images, audio, video)

3. **Monitoring & Observability Bridge** (High):
   - Expose agent metrics collection to scripts
   - Add event subscription mechanisms
   - Enable alert configuration from scripts
   - Provide performance tracking access

4. **Lifecycle Management Bridge** (Medium):
   - Expose state machine transitions
   - Add lifecycle event hooks
   - Enable resource management from scripts
   - Support graceful shutdown patterns

5. **Enhanced Context Bridge** (Medium):
   - Create hierarchical context support
   - Enable context inheritance
   - Add shared memory regions
   - Integrate with event bus

6. **Composition Pattern Bridge** (Medium):
   - Expose hierarchical composition
   - Enable delegation patterns
   - Support capability aggregation
   - Add pipeline composition

7. **Workflow Integration** (Low - Phase 3.3.16):
   - Create WorkflowBridge similar to AgentBridge
   - Add workflow discovery and execution from scripts
   - Enable agents to participate in workflows via bridge
   - Support multi-agent coordination

8. **Performance Optimization** (Low):
   - Implement caching for frequently accessed agents
   - Add connection pooling for agent instances
   - Optimize parameter conversion for large data structures
   - Profile and optimize bridge overhead to <10ms

### 8. Agent Composition Patterns

```rust
// llmspell-agents/src/composition.rs
pub trait AgentComposition {
    fn compose(agents: Vec<Box<dyn Agent>>) -> Result<Box<dyn Agent>>;
}

// Pipeline Composition: Agents process in sequence
pub struct PipelineComposition {
    agents: Vec<Box<dyn Agent>>,
}

impl Agent for PipelineComposition {
    async fn process(&self, input: AgentInput) -> Result<AgentOutput> {
        let mut current = input;
        
        for agent in &self.agents {
            let output = agent.process(current).await?;
            current = AgentInput::from_output(output)?;
        }
        
        Ok(current.into_output())
    }
}

// Ensemble Composition: Multiple agents vote on result
pub struct EnsembleComposition {
    agents: Vec<Box<dyn Agent>>,
    voting_strategy: VotingStrategy,
}

impl Agent for EnsembleComposition {
    async fn process(&self, input: AgentInput) -> Result<AgentOutput> {
        let mut results = Vec::new();
        
        // Process in parallel
        let futures: Vec<_> = self.agents.iter()
            .map(|agent| agent.process(input.clone()))
            .collect();
        
        let outputs = futures::future::join_all(futures).await;
        
        for output in outputs {
            if let Ok(result) = output {
                results.push(result);
            }
        }
        
        // Apply voting strategy
        self.voting_strategy.select(results)
    }
}

// Hierarchical Composition: Parent agent delegates to children
pub struct HierarchicalComposition {
    parent: Box<dyn Agent>,
    children: HashMap<String, Box<dyn Agent>>,
    delegation_strategy: DelegationStrategy,
}

impl Agent for HierarchicalComposition {
    async fn process(&self, input: AgentInput) -> Result<AgentOutput> {
        // Parent analyzes and decides delegation
        let analysis = self.parent.process(input.clone()).await?;
        
        // Determine which child agents to use
        let delegations = self.delegation_strategy.decide(&analysis)?;
        
        // Execute delegated tasks
        let mut results = HashMap::new();
        for (task, agent_name) in delegations {
            if let Some(agent) = self.children.get(&agent_name) {
                let result = agent.process(task).await?;
                results.insert(agent_name, result);
            }
        }
        
        // Parent aggregates results
        self.parent.aggregate(results).await
}
```

### 9. Agent Examples

#### 9.1 Research Agent Configuration

```rust
let research_agent_spec = AgentSpec {
    name: "research_assistant".to_string(),
    agent_type: "research".to_string(),
    description: Some("Agent for conducting web research and analysis".to_string()),
    tools: vec![
        "web_search".to_string(),
        "web_scraper".to_string(),
        "text_summarizer".to_string(),
        "sentiment_analyzer".to_string(),
        "file_writer".to_string(),
        "json_processor".to_string(),
    ],
    llm_provider: Some(LlmProviderSpec {
        provider: "openai".to_string(),
        model: "gpt-4".to_string(),
        config: Some(json!({
            "temperature": 0.7,
            "max_tokens": 2000,
        })),
    }),
    config: Some(AgentConfig {
        max_iterations: 20,
        timeout: Duration::from_secs(300),
        temperature: 0.7,
        max_tokens: 2000,
        retry_policy: RetryPolicy::exponential(3),
        memory_config: MemoryConfig::conversational(10),
        security_config: SecurityConfig::standard(),
    }),
    metadata: hashmap!{
        "capabilities".to_string() => json!(["research", "analysis", "summarization"]),
        "version".to_string() => json!("1.0"),
    },
};

// Create agent using factory
let agent = factory.create_agent(&research_agent_spec).await?;

// Use the agent
let result = agent.process(AgentInput {
    message: "Research the latest developments in quantum computing".to_string(),
    context: Default::default(),
}).await?;
```

#### 9.2 Code Generation Agent with Composition

```rust
// Create a hierarchical code agent with specialized sub-agents
let code_agent = HierarchicalComposition {
    parent: factory.create_agent(&AgentSpec {
        name: "code_orchestrator".to_string(),
        agent_type: "orchestrator".to_string(),
        tools: vec!["code_analyzer".to_string()],
        ..Default::default()
    }).await?,
    
    children: hashmap!{
        "generator".to_string() => factory.create_agent(&AgentSpec {
            name: "code_generator".to_string(),
            agent_type: "code".to_string(),
            tools: vec!["file_operations", "template_engine"],
            config: Some(AgentConfig {
                temperature: 0.2,
                ..Default::default()
            }),
            ..Default::default()
        }).await?,
        
        "tester".to_string() => factory.create_agent(&AgentSpec {
            name: "test_generator".to_string(),
            agent_type: "code".to_string(),
            tools: vec!["test_runner", "file_operations"],
            ..Default::default()
        }).await?,
        
        "documenter".to_string() => factory.create_agent(&AgentSpec {
            name: "doc_generator".to_string(),
            agent_type: "documentation".to_string(),
            tools: vec!["markdown_processor", "template_engine"],
            ..Default::default()
        }).await?,
    },
    
    delegation_strategy: DelegationStrategy::TaskBased,
};
```

#### 9.3 Data Processing Pipeline

```rust
// Create a pipeline of agents for data processing
let data_pipeline = PipelineComposition {
    agents: vec![
        // Stage 1: Data extraction
        factory.create_agent(&AgentSpec {
            name: "data_extractor".to_string(),
            agent_type: "data".to_string(),
            tools: vec!["database_connector", "csv_analyzer"],
            ..Default::default()
        }).await?,
        
        // Stage 2: Data validation
        factory.create_agent(&AgentSpec {
            name: "data_validator".to_string(),
            agent_type: "validation".to_string(),
            tools: vec!["data_validator", "json_processor"],
            ..Default::default()
        }).await?,
        
        // Stage 3: Data transformation
        factory.create_agent(&AgentSpec {
            name: "data_transformer".to_string(),
            agent_type: "transform".to_string(),
            tools: vec!["json_processor", "template_engine"],
            ..Default::default()
        }).await?,
        
        // Stage 4: Data storage
        factory.create_agent(&AgentSpec {
            name: "data_writer".to_string(),
            agent_type: "storage".to_string(),
            tools: vec!["file_writer", "database_connector"],
            ..Default::default()
        }).await?,
    ],
};
```

### 10. Basic Workflow Patterns

Basic workflow patterns that leverage current Phase 3 infrastructure without requiring persistent state, hooks, or sessions from later phases.

#### 10.1 Workflow Architecture (Flat Structure)

**File Structure** (Refactored to flat hierarchy):
```
llmspell-workflows/src/
├── lib.rs               # Public API exports
├── traits.rs            # Core traits (ErrorStrategy, StepResult, etc.)
├── types.rs             # Workflow types (WorkflowConfig, WorkflowState, etc.)
├── state.rs             # Memory-based state management
├── step_executor.rs     # Step execution engine
├── error_handling.rs    # Error strategies and recovery
├── conditions.rs        # Condition evaluation for workflows
├── sequential.rs        # Sequential workflow implementation
└── conditional.rs       # Conditional workflow implementation
```

**Core Workflow Traits**:
```rust
// llmspell-workflows/src/traits.rs
#[async_trait]
pub trait Workflow: Send + Sync {
    /// Get workflow name
    fn name(&self) -> &str;
    
    /// Get workflow status
    async fn status(&self) -> Result<WorkflowStatus>;
    
    /// Execute workflow
    async fn execute(&self) -> Result<WorkflowOutput>;
    
    /// Reset workflow state
    async fn reset(&self) -> Result<()>;
}

#[derive(Debug, Clone)]
pub struct WorkflowInput {
    pub initial_data: Value,
    pub parameters: HashMap<String, Value>,
}

#[derive(Debug, Clone)]
pub struct WorkflowOutput {
    pub final_result: Value,
    pub step_results: Vec<StepResult>,
    pub execution_path: Vec<String>,
}

#[derive(Debug, Clone)]
pub struct StepResult {
    pub step_id: String,
    pub result: Value,
    pub status: StepStatus,
    pub duration: Duration,
}

#[derive(Debug, Clone)]
pub enum StepStatus {
    Success,
    Failed(String),
    Skipped,
}
```

#### 10.2 Basic Sequential Workflow

```rust
// Simple sequential execution using tools and agents
pub struct SequentialWorkflow {
    id: String,
    name: String,
    steps: Vec<WorkflowStep>,
    error_handling: ErrorStrategy,
}

#[derive(Debug, Clone)]
pub struct WorkflowStep {
    pub id: String,
    pub step_type: StepType,
    pub parameters: Value,
}

#[derive(Debug, Clone)]
pub enum StepType {
    Tool(String),              // Execute a tool
    Agent(String),             // Execute an agent  
    Transform(String),         // Data transformation
}

#[derive(Debug, Clone)]
pub enum ErrorStrategy {
    Fail,                      // Stop on first error
    Continue,                  // Skip failed steps
    Retry(usize),             // Retry failed steps
}

#[async_trait]
impl Workflow for SequentialWorkflow {
    async fn execute(&self, input: WorkflowInput, context: &ExecutionContext) -> Result<WorkflowOutput> {
        let mut current_data = input.initial_data;
        let mut step_results = Vec::new();
        let mut execution_path = Vec::new();
        
        for step in &self.steps {
            execution_path.push(step.id.clone());
            let start_time = Instant::now();
            
            let result = match &step.step_type {
                StepType::Tool(tool_name) => {
                    self.execute_tool(tool_name, &step.parameters, &current_data, context).await
                }
                StepType::Agent(agent_name) => {
                    self.execute_agent(agent_name, &step.parameters, &current_data, context).await
                }
                StepType::Transform(transform_name) => {
                    self.execute_transform(transform_name, &step.parameters, &current_data).await
                }
            };
            
            let duration = start_time.elapsed();
            
            match result {
                Ok(value) => {
                    current_data = value.clone();
                    step_results.push(StepResult {
                        step_id: step.id.clone(),
                        result: value,
                        status: StepStatus::Success,
                        duration,
                    });
                }
                Err(e) => {
                    let error_msg = e.to_string();
                    step_results.push(StepResult {
                        step_id: step.id.clone(),
                        result: Value::Null,
                        status: StepStatus::Failed(error_msg.clone()),
                        duration,
                    });
                    
                    match &self.error_handling {
                        ErrorStrategy::Fail => return Err(e),
                        ErrorStrategy::Continue => continue,
                        ErrorStrategy::Retry(attempts) => {
                            // Implement basic retry logic
                            for attempt in 1..=*attempts {
                                tokio::time::sleep(Duration::from_millis(100 * attempt as u64)).await;
                                if let Ok(retry_result) = self.retry_step(step, &current_data, context).await {
                                    current_data = retry_result;
                                    break;
                                }
                            }
                        }
                    }
                }
            }
        }
        
        Ok(WorkflowOutput {
            final_result: current_data,
            step_results,
            execution_path,
        })
    }
}
```

#### 10.3 Basic Conditional Workflow

```rust
// Simple conditional logic using memory-based conditions
pub struct ConditionalWorkflow {
    id: String,
    name: String,
    initial_step: String,
    steps: HashMap<String, ConditionalStep>,
    conditions: HashMap<String, Condition>,
}

#[derive(Debug, Clone)]
pub struct ConditionalStep {
    pub id: String,
    pub step_type: StepType,
    pub parameters: Value,
    pub branches: Vec<ConditionalBranch>,
}

#[derive(Debug, Clone)]
pub struct ConditionalBranch {
    pub condition: String,
    pub next_step: Option<String>,
}

#[derive(Debug, Clone)]
pub enum Condition {
    ValueEquals(String, Value),          // data.field == value
    ValueGreaterThan(String, f64),       // data.field > value
    ValueContains(String, String),       // data.field contains string
    ResultSuccess,                       // previous step succeeded
    Custom(String),                      // custom condition function
}

impl Condition {
    pub fn evaluate(&self, data: &Value, step_result: &StepResult) -> Result<bool> {
        match self {
            Condition::ValueEquals(path, expected) => {
                let actual = self.extract_value(data, path)?;
                Ok(actual == *expected)
            }
            Condition::ValueGreaterThan(path, threshold) => {
                let actual = self.extract_value(data, path)?;
                if let Some(num) = actual.as_f64() {
                    Ok(num > *threshold)
                } else {
                    Ok(false)
                }
            }
            Condition::ValueContains(path, substring) => {
                let actual = self.extract_value(data, path)?;
                if let Some(text) = actual.as_str() {
                    Ok(text.contains(substring))
                } else {
                    Ok(false)
                }
            }
            Condition::ResultSuccess => {
                Ok(matches!(step_result.status, StepStatus::Success))
            }
            Condition::Custom(func_name) => {
                // Custom condition evaluation
                self.evaluate_custom_condition(func_name, data, step_result)
            }
        }
    }
}
```

#### 10.4 Basic Loop Workflow

```rust
// Simple iteration patterns without persistent state
pub struct LoopWorkflow {
    id: String,
    name: String,
    iterator: Iterator,
    body_steps: Vec<WorkflowStep>,
    max_iterations: usize,
    break_condition: Option<Condition>,
}

#[derive(Debug, Clone)]
pub enum Iterator {
    Collection(Vec<Value>),              // Iterate over in-memory collection
    Range(usize, usize),                // Iterate over numeric range
    WhileCondition(Condition),     // While condition is true
}

#[async_trait]
impl Workflow for LoopWorkflow {
    async fn execute(&self, input: WorkflowInput, context: &ExecutionContext) -> Result<WorkflowOutput> {
        let mut iterations = 0;
        let mut all_results = Vec::new();
        let mut execution_path = Vec::new();
        
        loop {
            // Check max iterations
            if iterations >= self.max_iterations {
                break;
            }
            
            // Get next item
            let (should_continue, item) = match &self.iterator {
                Iterator::Collection(items) => {
                    if iterations < items.len() {
                        (true, items[iterations].clone())
                    } else {
                        (false, Value::Null)
                    }
                }
                Iterator::Range(start, end) => {
                    let current = start + iterations;
                    if current < *end {
                        (true, Value::Number(current.into()))
                    } else {
                        (false, Value::Null)
                    }
                }
                Iterator::WhileCondition(condition) => {
                    // Evaluate condition with current context
                    if iterations == 0 {
                        (true, input.initial_data.clone())
                    } else {
                        let dummy_result = StepResult {
                            step_id: "loop_check".to_string(),
                            result: Value::Null,
                            status: StepStatus::Success,
                            duration: Duration::ZERO,
                        };
                        let should_continue = condition.evaluate(&input.initial_data, &dummy_result)?;
                        (should_continue, input.initial_data.clone())
                    }
                }
            };
            
            if !should_continue {
                break;
            }
            
            // Check break condition
            if let Some(break_cond) = &self.break_condition {
                let dummy_result = StepResult {
                    step_id: "break_check".to_string(),
                    result: Value::Null,
                    status: StepStatus::Success,
                    duration: Duration::ZERO,
                };
                if break_cond.evaluate(&item, &dummy_result)? {
                    break;
                }
            }
            
            // Execute body steps with current item
            let mut current_data = item;
            for step in &self.body_steps {
                execution_path.push(format!("{}_{}", step.id, iterations));
                
                let result = match &step.step_type {
                    StepType::Tool(tool_name) => {
                        self.execute_tool(tool_name, &step.parameters, &current_data, context).await?
                    }
                    StepType::Agent(agent_name) => {
                        self.execute_agent(agent_name, &step.parameters, &current_data, context).await?
                    }
                    StepType::Transform(transform_name) => {
                        self.execute_transform(transform_name, &step.parameters, &current_data).await?
                    }
                };
                
                current_data = result;
            }
            
            all_results.push(current_data);
            iterations += 1;
        }
        
        Ok(WorkflowOutput {
            final_result: Value::Array(all_results),
            step_results: vec![], // Simplified for basic implementation
            execution_path,
        })
    }
}
```

#### 10.5 Basic Parallel Workflow

```rust
// Simple parallel execution without advanced features
pub struct ParallelWorkflow {
    id: String,
    name: String,
    branches: Vec<ParallelBranch>,
    max_concurrency: usize,  // Fixed at creation
    error_handling: ErrorStrategy,
}

#[derive(Debug, Clone)]
pub struct ParallelBranch {
    pub id: String,
    pub name: String,
    pub step_type: StepType,
    pub required: bool,  // If true, failure fails the entire workflow
}

#[async_trait]
impl Workflow for ParallelWorkflow {
    async fn execute(&self, input: WorkflowInput, context: &ExecutionContext) -> Result<WorkflowOutput> {
        let mut branch_handles = Vec::new();
        
        // Fork: Start all branches concurrently
        for branch in &self.branches {
            let branch_context = context.clone();
            let branch_input = input.clone();
            
            let handle = tokio::spawn(async move {
                Self::execute_branch(branch, branch_input, &branch_context).await
            });
            
            branch_handles.push((branch.id.clone(), branch.required, handle));
        }
        
        // Join: Wait for all branches to complete
        let mut results = Vec::new();
        let mut errors = Vec::new();
        
        for (branch_id, required, handle) in branch_handles {
            match handle.await {
                Ok(Ok(result)) => {
                    results.push(StepResult {
                        step_id: branch_id,
                        success: true,
                        output: Some(result),
                        error: None,
                        execution_time: Duration::from_millis(0), // Simplified
                    });
                }
                Ok(Err(e)) | Err(e) => {
                    let error = format!("Branch {} failed: {:?}", branch_id, e);
                    errors.push(error.clone());
                    
                    results.push(StepResult {
                        step_id: branch_id.clone(),
                        success: false,
                        output: None,
                        error: Some(error),
                        execution_time: Duration::from_millis(0),
                    });
                    
                    // Fail-fast on required branch failure
                    if required {
                        return Err(LLMSpellError::WorkflowExecutionFailed(
                            format!("Required branch {} failed", branch_id)
                        ));
                    }
                }
            }
        }
        
        // Aggregate results (simple concatenation for basic implementation)
        let final_result = Self::aggregate_results(&results)?;
        
        Ok(WorkflowOutput {
            final_result,
            step_results: results,
            metadata: json!({
                "workflow_type": "parallel",
                "total_branches": self.branches.len(),
                "successful_branches": results.iter().filter(|r| r.success).count(),
                "failed_branches": errors.len()
            }),
            execution_path: vec![], // Simplified for basic implementation
        })
    }
}

impl ParallelWorkflow {
    pub fn builder(name: &str) -> ParallelWorkflowBuilder {
        ParallelWorkflowBuilder::new(name)
    }
    
    async fn execute_branch(
        branch: &ParallelBranch,
        input: WorkflowInput,
        context: &ExecutionContext
    ) -> Result<Value> {
        match &branch.step_type {
            StepType::Agent(agent_name) => {
                // Execute agent step
                let agent_input = AgentInput {
                    message: input.initial_data.to_string(),
                    context: context.clone(),
                    parameters: input.parameters,
                };
                
                // This would use the agent registry to get and execute the agent
                // Simplified for basic implementation
                Ok(json!({"agent_result": format!("Executed agent: {}", agent_name)}))
            }
            StepType::Tool(tool_name) => {
                // Execute tool step
                // This would use the tool registry
                Ok(json!({"tool_result": format!("Executed tool: {}", tool_name)}))
            }
            StepType::Workflow(workflow_name) => {
                // Execute nested workflow
                Ok(json!({"workflow_result": format!("Executed workflow: {}", workflow_name)}))
            }
        }
    }
    
    fn aggregate_results(results: &[StepResult]) -> Result<Value> {
        let mut aggregated = json!({});
        
        for result in results {
            if let Some(output) = &result.output {
                aggregated[&result.step_id] = output.clone();
            }
        }
        
        Ok(aggregated)
    }
}

// Builder pattern for ParallelWorkflow
pub struct ParallelWorkflowBuilder {
    name: String,
    branches: Vec<ParallelBranch>,
    max_concurrency: usize,
    error_handling: ErrorStrategy,
}

impl ParallelWorkflowBuilder {
    pub fn new(name: &str) -> Self {
        Self {
            name: name.to_string(),
            branches: Vec::new(),
            max_concurrency: 10,  // Reasonable default
            error_handling: ErrorStrategy::Fail,
        }
    }
    
    pub fn add_agent_branch(mut self, id: &str, agent_name: &str, required: bool) -> Self {
        self.branches.push(ParallelBranch {
            id: id.to_string(),
            name: id.to_string(),
            step_type: StepType::Agent(agent_name.to_string()),
            required,
        });
        self
    }
    
    pub fn add_tool_branch(mut self, id: &str, tool_name: &str, required: bool) -> Self {
        self.branches.push(ParallelBranch {
            id: id.to_string(),
            name: id.to_string(),
            step_type: StepType::Tool(tool_name.to_string()),
            required,
        });
        self
    }
    
    pub fn with_max_concurrency(mut self, max: usize) -> Self {
        self.max_concurrency = max;
        self
    }
    
    pub fn with_error_handling(mut self, strategy: BasicErrorStrategy) -> Self {
        self.error_handling = strategy;
        self
    }
    
    pub fn build(self) -> ParallelWorkflow {
        ParallelWorkflow {
            id: uuid::Uuid::new_v4().to_string(),
            name: self.name,
            branches: self.branches,
            max_concurrency: self.max_concurrency,
            error_handling: self.error_handling,
        }
    }
}
```

#### 10.6 Workflow-Agent Integration

```rust
// Agents can execute workflows
pub struct WorkflowAgent {
    id: String,
    name: String,
    workflow_registry: Arc<BasicWorkflowRegistry>,
    default_workflow: Option<String>,
}

#[async_trait]
impl Agent for WorkflowAgent {
    async fn process(&self, input: AgentInput) -> Result<AgentOutput> {
        // Determine which workflow to use
        let workflow_name = input.parameters.get("workflow")
            .and_then(|v| v.as_str())
            .or(self.default_workflow.as_deref())
            .ok_or_else(|| LLMSpellError::InvalidInput("No workflow specified".to_string()))?;
        
        // Get workflow from registry
        let workflow = self.workflow_registry.get(workflow_name)
            .ok_or_else(|| LLMSpellError::NotFound(format!("Workflow: {}", workflow_name)))?;
        
        // Execute workflow
        let workflow_input = WorkflowInput {
            initial_data: input.message.into(),
            parameters: input.parameters,
        };
        
        let result = workflow.execute(workflow_input, &input.context).await?;
        
        Ok(AgentOutput {
            content: result.final_result.to_string(),
            metadata: Some(json!({
                "workflow_name": workflow_name,
                "execution_path": result.execution_path,
                "step_count": result.step_results.len()
            })),
            context: input.context,
        })
    }
}

// Workflows can use agents as steps
impl SequentialWorkflow {
    async fn execute_agent(
        &self,
        agent_name: &str,
        parameters: &Value,
        data: &Value,
        context: &ExecutionContext,
    ) -> Result<Value> {
        // Get agent from context
        let agent = context.agent_locator.find(agent_name)
            .ok_or_else(|| LLMSpellError::NotFound(format!("Agent: {}", agent_name)))?;
        
        // Prepare agent input
        let agent_input = AgentInput {
            message: data.to_string(),
            parameters: parameters.as_object().unwrap_or(&serde_json::Map::new()).clone(),
            context: context.clone(),
        };
        
        // Execute agent
        let result = agent.process(agent_input).await?;
        
        // Return result as JSON value
        Ok(json!({
            "content": result.content,
            "metadata": result.metadata
        }))
    }
}
```

#### 10.6 Basic Workflow Registry

```rust
// Simple in-memory workflow registry for Phase 3
pub struct BasicWorkflowRegistry {
    workflows: HashMap<String, Box<dyn BasicWorkflow>>,
}

impl BasicWorkflowRegistry {
    pub fn new() -> Self {
        Self {
            workflows: HashMap::new(),
        }
    }
    
    pub fn register<W: BasicWorkflow + 'static>(&mut self, workflow: W) {
        self.workflows.insert(workflow.id().to_string(), Box::new(workflow));
    }
    
    pub fn get(&self, id: &str) -> Option<&dyn BasicWorkflow> {
        self.workflows.get(id).map(|w| w.as_ref())
    }
    
    pub fn list(&self) -> Vec<String> {
        self.workflows.keys().cloned().collect()
    }
    
    pub fn remove(&mut self, id: &str) -> Option<Box<dyn BasicWorkflow>> {
        self.workflows.remove(id)
    }
}
```

### 11. Comprehensive Script Integration for Workflows

This section describes the complete script integration layer that provides comprehensive workflow capabilities through pre-injected global objects, following the rs-llmspell architecture vision.

#### 11.1 Global Workflow Object Injection

Following the global injection pattern, the `Workflow` object provides all workflow functionality without requiring imports:

```rust
// llmspell-bridge/src/globals/workflow_global.rs
pub struct WorkflowGlobal {
    workflow_registry: Arc<BasicWorkflowRegistry>,
    workflow_factory: Arc<WorkflowFactory>,
    state_manager: Arc<StateManager>,
    hook_manager: Arc<HookManager>,
}

impl WorkflowGlobal {
    pub fn new(
        workflow_registry: Arc<BasicWorkflowRegistry>,
        workflow_factory: Arc<WorkflowFactory>,
        state_manager: Arc<StateManager>,
        hook_manager: Arc<HookManager>,
    ) -> Self {
        Self {
            workflow_registry,
            workflow_factory,
            state_manager,
            hook_manager,
        }
    }
    
    // Register the global Workflow object with the script engine
    pub fn inject_into_lua(&self, lua: &Lua) -> Result<()> {
        let workflow_table = lua.create_table()?;
        
        // Workflow creation methods
        workflow_table.set("sequential", self.create_sequential_constructor(lua)?)?;
        workflow_table.set("parallel", self.create_parallel_constructor(lua)?)?;
        workflow_table.set("conditional", self.create_conditional_constructor(lua)?)?;
        workflow_table.set("loop", self.create_loop_constructor(lua)?)?;
        
        // Workflow management methods
        workflow_table.set("list", self.create_list_function(lua)?)?;
        workflow_table.set("get", self.create_get_function(lua)?)?;
        workflow_table.set("register", self.create_register_function(lua)?)?;
        workflow_table.set("execute", self.create_execute_function(lua)?)?;
        
        // Discovery and introspection
        workflow_table.set("types", self.create_types_function(lua)?)?;
        workflow_table.set("info", self.create_info_function(lua)?)?;
        
        lua.globals().set("Workflow", workflow_table)?;
        
        Ok(())
    }
}
```

#### 11.2 Complete Lua Workflow API

The Lua API provides full workflow creation and management capabilities:

```lua
-- Pre-injected global: Workflow
-- No require() needed - always available

-- 1. Sequential Workflow Creation
local data_pipeline = Workflow.sequential({
    name = "data_processing_pipeline",
    description = "Multi-stage data processing workflow",
    
    steps = {
        {
            name = "extract_data",
            component = Tools.get("csv_analyzer"),
            input = function(context)
                return {
                    file_path = context.input.file_path,
                    headers = true,
                    delimiter = ","
                }
            end,
            output = "raw_data",
            error_strategy = "continue" -- or "abort", "retry"
        },
        
        {
            name = "clean_data",
            component = Agent.get("data_cleaner"),
            input = function(context)
                return {
                    data = context.raw_data,
                    remove_nulls = true,
                    standardize_formats = true
                }
            end,
            output = "clean_data"
        },
        
        {
            name = "analyze_patterns",
            component = Agent.get("pattern_analyzer"),
            input = function(context)
                return {
                    data = context.clean_data,
                    analysis_type = "trend_detection",
                    confidence_threshold = 0.85
                }
            end,
            output = "analysis_results"
        },
        
        {
            name = "generate_report",
            component = Tools.get("template_engine"),
            input = function(context)
                return {
                    template = "analysis_report.html",
                    data = {
                        raw_data_summary = context.raw_data.summary,
                        analysis = context.analysis_results,
                        timestamp = os.time()
                    }
                }
            end,
            output = "final_report"
        }
    },
    
    -- Error handling configuration
    error_strategy = {
        default = "abort",
        retries = 3,
        retry_delay = 1000, -- milliseconds
        on_error = function(step_name, error, context)
            Logger.error("Step failed: " .. step_name .. " - " .. error)
            -- Optional: cleanup or notification logic
        end
    },
    
    -- Hooks integration
    hooks = {
        before_start = function(context)
            Logger.info("Starting data pipeline for: " .. context.input.file_path)
            State.set("workflow_start_time", os.time())
        end,
        
        after_step = function(step_name, result, context)
            Logger.debug("Completed step: " .. step_name)
            State.set("last_completed_step", step_name)
        end,
        
        on_complete = function(result, context)
            local duration = os.time() - State.get("workflow_start_time")
            Logger.info("Data pipeline completed in " .. duration .. " seconds")
            Event.emit("workflow_completed", {
                workflow_name = "data_processing_pipeline",
                duration = duration,
                success = true
            })
        end
    }
})

-- 2. Parallel Workflow Creation
local multi_source_analysis = Workflow.parallel({
    name = "multi_source_analysis",
    description = "Analyze data from multiple sources simultaneously",
    
    branches = {
        {
            name = "social_media_analysis",
            component = Agent.get("social_media_analyst"),
            input = function(context)
                return {
                    topic = context.input.topic,
                    platforms = {"twitter", "reddit", "linkedin"},
                    time_range = "7d"
                }
            end,
            required = true, -- Failure of this branch fails the entire workflow
            timeout = 30000 -- 30 seconds
        },
        
        {
            name = "news_analysis", 
            component = Agent.get("news_analyst"),
            input = function(context)
                return {
                    topic = context.input.topic,
                    sources = {"reuters", "ap", "bloomberg"},
                    time_range = "7d"
                }
            end,
            required = true,
            timeout = 45000
        },
        
        {
            name = "academic_analysis",
            component = Agent.get("academic_researcher"),
            input = function(context)
                return {
                    topic = context.input.topic,
                    databases = {"pubmed", "arxiv", "scholar"},
                    time_range = "30d"
                }
            end,
            required = false, -- Optional - failures don't fail workflow
            timeout = 60000
        }
    },
    
    -- Parallel execution configuration
    max_concurrency = 3,
    wait_for_all = true, -- or false to return as soon as required branches complete
    
    -- Result aggregation
    aggregation = {
        strategy = "merge", -- "merge", "array", "custom"
        merge_key = "source_type",
        custom_aggregator = function(results)
            local merged = {
                social_data = results.social_media_analysis,
                news_data = results.news_analysis,
                academic_data = results.academic_analysis,
                analysis_timestamp = os.time(),
                total_sources = 0
            }
            
            -- Count total sources across all analyses
            for _, result in pairs(results) do
                if result and result.source_count then
                    merged.total_sources = merged.total_sources + result.source_count
                end
            end
            
            return merged
        end
    },
    
    -- Error handling for parallel execution
    error_strategy = {
        required_failure = "abort", -- Abort if any required branch fails
        optional_failure = "continue", -- Continue if optional branches fail
        timeout_action = "continue_partial" -- Continue with successful branches on timeout
    }
})

-- 3. Conditional Workflow Creation
local adaptive_content_processor = Workflow.conditional({
    name = "adaptive_content_processor",
    description = "Process content with different strategies based on characteristics",
    
    condition = function(context)
        local content = context.input.content
        local content_length = string.len(content)
        local complexity_score = Agent.get("complexity_analyzer"):quick_score(content)
        
        return {
            is_complex = content_length > 5000 and complexity_score > 0.7,
            content_type = context.input.content_type or "text",
            priority = context.input.priority or "normal"
        }
    end,
    
    branches = {
        -- Complex content branch
        complex_processing = {
            condition = function(ctx) return ctx.condition_result.is_complex end,
            workflow = Workflow.sequential({
                name = "complex_content_processing",
                steps = {
                    {
                        name = "deep_analysis",
                        component = Agent.get("deep_content_analyzer"),
                        input = function(ctx) return {content = ctx.input.content, mode = "comprehensive"} end
                    },
                    {
                        name = "expert_review",
                        component = Agent.get("expert_reviewer"),
                        input = function(ctx) return {content = ctx.input.content, analysis = ctx.deep_analysis} end
                    },
                    {
                        name = "detailed_summary",
                        component = Agent.get("detailed_summarizer"),
                        input = function(ctx) return {content = ctx.input.content, expert_notes = ctx.expert_review} end
                    }
                }
            })
        },
        
        -- Simple content branch  
        simple_processing = {
            condition = function(ctx) return not ctx.condition_result.is_complex end,
            workflow = Workflow.sequential({
                name = "simple_content_processing",
                steps = {
                    {
                        name = "basic_analysis",
                        component = Agent.get("basic_content_analyzer"),
                        input = function(ctx) return {content = ctx.input.content, mode = "quick"} end
                    },
                    {
                        name = "quick_summary",
                        component = Agent.get("quick_summarizer"),
                        input = function(ctx) return {content = ctx.input.content, analysis = ctx.basic_analysis} end
                    }
                }
            })
        },
        
        -- High priority branch (can override complexity)
        priority_processing = {
            condition = function(ctx) return ctx.condition_result.priority == "high" end,
            workflow = Workflow.parallel({
                name = "priority_content_processing",
                branches = {
                    {name = "fast_analysis", component = Agent.get("fast_analyzer")},
                    {name = "priority_summary", component = Agent.get("priority_summarizer")}
                }
            })
        }
    },
    
    -- Default branch if no conditions match
    default_branch = "simple_processing"
})

-- 4. Loop Workflow Creation
local iterative_refinement = Workflow.loop({
    name = "iterative_content_refinement",
    description = "Iteratively refine content until quality threshold is met",
    
    -- Loop condition
    condition = function(context, iteration)
        if iteration >= 5 then
            return false -- Max 5 iterations
        end
        
        if iteration == 0 then
            return true -- Always run at least once
        end
        
        -- Continue if quality score is below threshold
        local quality_score = context.last_result.quality_score or 0
        return quality_score < 0.9
    end,
    
    -- Loop body
    body = Workflow.sequential({
        name = "refinement_iteration",
        steps = {
            {
                name = "analyze_quality",
                component = Agent.get("quality_analyzer"),
                input = function(context)
                    local content = context.iteration == 0 and context.input.content or context.last_result.refined_content
                    return {
                        content = content,
                        quality_metrics = {"clarity", "completeness", "accuracy", "engagement"}
                    }
                end,
                output = "quality_analysis"
            },
            
            {
                name = "identify_improvements",
                component = Agent.get("improvement_identifier"),
                input = function(context)
                    return {
                        content = context.iteration == 0 and context.input.content or context.last_result.refined_content,
                        quality_analysis = context.quality_analysis,
                        previous_improvements = context.improvement_history or {}
                    }
                end,
                output = "improvement_suggestions"
            },
            
            {
                name = "apply_refinements",
                component = Agent.get("content_refiner"),
                input = function(context)
                    return {
                        content = context.iteration == 0 and context.input.content or context.last_result.refined_content,
                        improvements = context.improvement_suggestions,
                        style_guide = context.input.style_guide
                    }
                end,
                output = "refined_content"
            },
            
            {
                name = "validate_improvements",
                component = Agent.get("improvement_validator"),
                input = function(context)
                    return {
                        original_content = context.iteration == 0 and context.input.content or context.last_result.refined_content,
                        refined_content = context.refined_content,
                        quality_analysis = context.quality_analysis
                    }
                end,
                output = "validation_result"
            }
        }
    }),
    
    -- Iteration state management
    state_management = {
        preserve_between_iterations = {"improvement_history", "quality_trend"},
        update_on_iteration = function(context, iteration_result)
            -- Track improvement history
            if not context.improvement_history then
                context.improvement_history = {}
            end
            table.insert(context.improvement_history, iteration_result.improvement_suggestions)
            
            -- Track quality trend
            if not context.quality_trend then
                context.quality_trend = {}
            end
            table.insert(context.quality_trend, iteration_result.validation_result.quality_score)
            
            return context
        end
    },
    
    -- Loop completion handling
    on_complete = function(context, iterations, final_result)
        Logger.info("Content refinement completed after " .. iterations .. " iterations")
        
        return {
            final_content = final_result.refined_content,
            quality_score = final_result.validation_result.quality_score,
            iterations_performed = iterations,
            improvement_history = context.improvement_history,
            quality_progression = context.quality_trend
        }
    end
})

-- 5. Workflow Execution and Management

-- Execute workflows
local pipeline_result = data_pipeline:execute({
    file_path = "data/customer_feedback.csv",
    output_format = "html"
})

local analysis_result = multi_source_analysis:execute({
    topic = "sustainable energy trends",
    region = "global"
})

local processed_content = adaptive_content_processor:execute({
    content = "Long technical document content...",
    content_type = "technical",
    priority = "normal"
})

local refined_content = iterative_refinement:execute({
    content = "Draft content to be improved...",
    style_guide = "academic",
    target_quality = 0.95
})

-- Workflow registry management
Workflow.register("my_data_pipeline", data_pipeline)
Workflow.register("multi_analysis", multi_source_analysis)

-- List and discover workflows
local available_workflows = Workflow.list()
for _, workflow_info in ipairs(available_workflows) do
    print("Available workflow: " .. workflow_info.name .. " - " .. workflow_info.description)
end

-- Get workflow information
local workflow_info = Workflow.info("my_data_pipeline")
print("Workflow type: " .. workflow_info.workflow_type)
print("Expected parameters: " .. JSON.stringify(workflow_info.parameters))

-- List workflow types
local workflow_types = Workflow.types()
for _, type_info in ipairs(workflow_types) do
    print("Workflow type: " .. type_info.name .. " - " .. type_info.description)
end
```

#### 11.3 State and Hook Integration

Workflows integrate seamlessly with the global State and Hook systems:

```lua
-- State management in workflows
local stateful_workflow = Workflow.sequential({
    name = "stateful_processing",
    steps = {
        {
            name = "load_state",
            component = function(context)
                -- Access shared state
                local previous_results = State.get("workflow_cache") or {}
                local user_preferences = State.get("user_prefs") or {}
                
                return {
                    cached_data = previous_results,
                    preferences = user_preferences
                }
            end
        },
        
        {
            name = "process_with_state",
            component = Agent.get("processor"),
            input = function(context)
                return {
                    data = context.input.data,
                    cached_results = context.load_state.cached_data,
                    user_preferences = context.load_state.preferences
                }
            end
        },
        
        {
            name = "save_state",
            component = function(context)
                -- Update shared state
                State.set("workflow_cache", context.process_with_state.results)
                State.set("last_execution", os.time())
                
                return {success = true}
            end
        }
    }
})

-- Hook integration with workflows
Hook.register("before_workflow_execution", function(event)
    local workflow_name = event.data.workflow_name
    Logger.info("Starting workflow: " .. workflow_name)
    
    -- Set up monitoring
    State.set("workflow_start_time_" .. workflow_name, os.time())
    
    -- Check resource availability
    if not System.check_resources() then
        error("Insufficient resources to start workflow")
    end
end)

Hook.register("after_workflow_step", function(event)
    local step_name = event.data.step_name
    local workflow_name = event.data.workflow_name
    local success = event.data.success
    
    -- Update step tracking
    local step_history = State.get("step_history") or {}
    table.insert(step_history, {
        workflow = workflow_name,
        step = step_name,
        success = success,
        timestamp = os.time()
    })
    State.set("step_history", step_history)
    
    -- Emit custom events for monitoring
    Event.emit("step_completed", {
        workflow = workflow_name,
        step = step_name,
        success = success
    })
end)

Hook.register("workflow_error", function(event)
    local workflow_name = event.data.workflow_name
    local error_message = event.data.error
    
    Logger.error("Workflow failed: " .. workflow_name .. " - " .. error_message)
    
    -- Cleanup resources
    State.remove("workflow_start_time_" .. workflow_name)
    
    -- Send notification
    Event.emit("workflow_failure", {
        workflow = workflow_name,
        error = error_message,
        timestamp = os.time()
    })
end)
```

#### 11.4 Advanced Workflow Composition

Workflows can be composed and nested for complex orchestrations:

```lua
-- Compose workflows into larger orchestrations
local master_research_workflow = Workflow.sequential({
    name = "comprehensive_research_pipeline",
    description = "Complete research pipeline with data gathering, analysis, and reporting",
    
    steps = {
        -- Step 1: Data gathering using parallel workflow
        {
            name = "data_gathering",
            component = multi_source_analysis, -- Reuse the parallel workflow
            input = function(context)
                return {
                    topic = context.input.research_topic,
                    depth = "comprehensive"
                }
            end,
            output = "gathered_data"
        },
        
        -- Step 2: Quality assessment using conditional workflow
        {
            name = "quality_assessment",
            component = adaptive_content_processor, -- Reuse conditional workflow
            input = function(context)
                return {
                    content = JSON.stringify(context.gathered_data),
                    content_type = "research_data",
                    priority = context.input.priority or "normal"
                }
            end,
            output = "quality_report"
        },
        
        -- Step 3: Iterative refinement if needed
        {
            name = "data_refinement", 
            component = function(context)
                local quality_score = context.quality_report.quality_score or 0
                
                if quality_score < 0.8 then
                    -- Use iterative refinement workflow
                    return iterative_refinement:execute({
                        content = JSON.stringify(context.gathered_data),
                        style_guide = "research",
                        target_quality = 0.9
                    })
                else
                    -- Data is already good quality
                    return {
                        final_content = context.gathered_data,
                        quality_score = quality_score,
                        iterations_performed = 0
                    }
                end
            end,
            output = "refined_data"
        },
        
        -- Step 4: Final analysis and report generation
        {
            name = "generate_final_report",
            component = Agent.get("research_report_generator"),
            input = function(context)
                return {
                    research_data = context.refined_data.final_content,
                    quality_metrics = context.quality_report,
                    research_topic = context.input.research_topic,
                    target_audience = context.input.target_audience,
                    format = context.input.output_format or "comprehensive"
                }
            end,
            output = "final_report"
        }
    },
    
    -- Master workflow hooks
    hooks = {
        before_start = function(context)
            Logger.info("Starting comprehensive research for: " .. context.input.research_topic)
            Event.emit("research_started", {
                topic = context.input.research_topic,
                requester = context.input.requester
            })
        end,
        
        on_complete = function(result, context)
            Logger.info("Research completed for: " .. context.input.research_topic)
            Event.emit("research_completed", {
                topic = context.input.research_topic,
                quality_score = result.quality_metrics.overall_score,
                data_sources = result.source_count
            })
        end
    }
})

-- Execute the master workflow
local research_result = master_research_workflow:execute({
    research_topic = "Impact of quantum computing on cybersecurity",
    target_audience = "enterprise_security_teams",
    priority = "high",
    output_format = "executive_briefing",
    requester = "security_team"
})

print("Research completed!")
print("Final report length: " .. string.len(research_result.final_report.content))
print("Quality score: " .. research_result.quality_metrics.overall_score)
print("Sources analyzed: " .. research_result.source_count)
```

#### 11.5 Performance and Bridge Architecture

The workflow bridge maintains the same performance characteristics as other bridges:

```rust
// Performance-optimized workflow bridge implementation
pub struct WorkflowBridge {
    workflow_registry: Arc<BasicWorkflowRegistry>,
    workflow_factory: Arc<WorkflowFactory>,
    state_manager: Arc<StateManager>,
    hook_manager: Arc<HookManager>,
    execution_cache: Arc<ExecutionCache>,
    performance_monitor: Arc<PerformanceMonitor>,
}

impl WorkflowBridge {
    // Performance Requirements:
    // - <10ms overhead for workflow creation
    // - <5ms overhead for workflow execution initiation  
    // - <2ms for workflow discovery operations
    // - Memory efficient parameter conversion
    // - Proper error handling with script error formatting
    
    pub async fn execute_workflow_from_script(
        &self,
        workflow_def: ScriptValue,
        input: ScriptValue,
        context: ScriptExecutionContext
    ) -> Result<ScriptValue> {
        let start = Instant::now();
        
        // Convert script workflow definition to native workflow
        let workflow = self.convert_script_workflow(workflow_def)?;
        
        // Convert input parameters
        let workflow_input = self.convert_script_input(input)?;
        let execution_context = self.convert_execution_context(context)?;
        
        // Execute workflow
        let result = workflow.execute(workflow_input, &execution_context).await?;
        
        // Convert result back to script
        let script_result = self.convert_workflow_output(result)?;
        
        // Record performance metrics
        let duration = start.elapsed();
        self.performance_monitor.record_workflow_execution(duration);
        
        Ok(script_result)
    }
}
```

**Performance Requirements:**
- **<10ms overhead** for workflow bridge operations
- **<5ms overhead** for workflow execution initiation
- **<2ms** for workflow discovery operations  
- **Consistent API** patterns across all bridge types
- **Memory efficient** parameter conversion
- **Error handling** with proper script error formatting

### 12. Lua Agent Examples

#### 10.1 Basic Agent Calling

```lua
-- examples/agents-basic.lua
local llmspell = require('llmspell')

-- Discover available agents
local agents = llmspell.agents.discover()
for _, agent in ipairs(agents) do
    print(string.format("Agent: %s - %s", agent.name, agent.description))
end

-- Create a research agent
local research_agent = llmspell.agents.create({
    name = "research_assistant",
    template = "research",
    tools = {"web_search", "web_scraper", "text_summarizer"},
    llm_provider = {
        provider = "openai",
        model = "gpt-4",
        temperature = 0.7
    }
})

-- Use the agent
local result = research_agent:process({
    message = "Research the latest developments in quantum computing",
    context = {
        depth = "comprehensive",
        sources = {"academic", "industry"}
    }
})

print("Research Result:", result.content)
if result.metadata then
    print("Sources found:", #result.metadata.sources)
end

-- Get agent information
local agent_info = llmspell.agents.get_info("research_assistant")
print("Agent capabilities:", table.concat(agent_info.capabilities, ", "))
print("Available tools:", table.concat(agent_info.tools, ", "))
```

#### 10.2 Agent Orchestration

```lua
-- examples/agents-composition.lua
local llmspell = require('llmspell')

-- Create multiple specialized agents
local research_agent = llmspell.agents.create({
    name = "researcher",
    template = "research",
    tools = {"web_search", "web_scraper"}
})

local analysis_agent = llmspell.agents.create({
    name = "analyzer",
    template = "analysis",
    tools = {"text_summarizer", "sentiment_analyzer"}
})

local writing_agent = llmspell.agents.create({
    name = "writer",
    template = "writing",
    tools = {"template_engine", "text_manipulator"}
})

-- Orchestrate agents in sequence
local function research_and_write(topic)
    -- Step 1: Research
    local research_result = research_agent:process({
        message = "Research information about " .. topic
    })
    
    if not research_result.success then
        error("Research failed: " .. (research_result.error or "Unknown error"))
    end
    
    -- Step 2: Analyze
    local analysis_result = analysis_agent:process({
        message = "Analyze this research data",
        context = {
            data = research_result.content
        }
    })
    
    if not analysis_result.success then
        error("Analysis failed: " .. (analysis_result.error or "Unknown error"))
    end
    
    -- Step 3: Write
    local writing_result = writing_agent:process({
        message = "Write a comprehensive report",
        context = {
            research = research_result.content,
            analysis = analysis_result.content,
            format = "markdown"
        }
    })
    
    return writing_result
end

-- Execute the orchestrated workflow
local report = research_and_write("artificial intelligence in healthcare")
print("Generated Report:", report.content)

-- Error handling example
local function safe_agent_call(agent, input)
    local success, result = pcall(function()
        return agent:process(input)
    end)
    
    if not success then
        print("Agent call failed:", result)
        return nil
    end
    
    if not result.success then
        print("Agent execution failed:", result.error)
        return nil
    end
    
    return result
end
```

#### 10.3 Agent Factory Usage

```lua
-- examples/agents-factory.lua
local llmspell = require('llmspell')

-- Get the agent factory
local factory = llmspell.agents.factory()

-- List available templates
local templates = factory:list_templates()
for _, template in ipairs(templates) do
    print(string.format("Template: %s - %s", template.name, template.description))
    print("  Required tools:", table.concat(template.required_tools, ", "))
end

-- Create agents using different templates
local agents = {
    researcher = factory:create({
        template = "research",
        tools = {"web_search", "web_scraper", "url_analyzer"},
        config = {
            max_iterations = 10,
            timeout = 300
        }
    }),
    
    coder = factory:create({
        template = "coding",
        tools = {"file_operations", "text_manipulator", "template_engine"},
        config = {
            language = "rust",
            style = "clean"
        }
    }),
    
    assistant = factory:create({
        template = "conversation",
        tools = {"calculator", "datetime_handler", "uuid_generator"},
        config = {
            personality = "helpful",
            response_length = "medium"
        }
    })
}

-- Use the agents
local code_result = agents.coder:process({
    message = "Create a simple HTTP server in Rust",
    context = {
        framework = "tokio",
        features = {"json", "cors"}
    }
})

print("Generated Code:", code_result.content)

-- Agent composition with factory
local composed_agent = factory:compose({
    agents = {"researcher", "coder"},
    pattern = "sequential",
    coordination = {
        pass_context = true,
        aggregate_results = true
    }
})

local composed_result = composed_agent:process({
    message = "Research Rust web frameworks and create a sample server"
})

print("Composed Result:", composed_result.content)
```

#### 10.4 Integration with Tools

```lua
-- examples/agents-tool-integration.lua
local llmspell = require('llmspell')

-- Create an agent that can use tools directly
local tool_agent = llmspell.agents.create({
    name = "tool_orchestrator",
    template = "tool_orchestrator",
    tools = {
        "calculator", "web_search", "file_operations", 
        "text_manipulator", "json_processor"
    }
})

-- Agent can call tools as part of its reasoning
local calculation_result = tool_agent:process({
    message = "Calculate the compound interest for $10,000 at 5% annually for 10 years, then search for current investment advice",
    context = {
        use_tools = true,
        steps = {
            "calculate_compound_interest",
            "search_investment_advice",
            "summarize_findings"
        }
    }
})

print("Tool Integration Result:", calculation_result.content)

-- Direct tool access for comparison
local calc_tool = llmspell.tools.calculator
local direct_result = calc_tool({
    operation = "evaluate",
    input = "10000 * (1 + 0.05)^10"
})

print("Direct Tool Result:", direct_result.result)

-- Agent can also be used as a tool by other agents
local meta_agent = llmspell.agents.create({
    name = "meta_orchestrator",
    template = "orchestrator"
})

-- Register the tool_agent as a tool for the meta_agent
meta_agent:register_tool("calculation_assistant", tool_agent)

local meta_result = meta_agent:process({
    message = "Use the calculation assistant to analyze multiple investment scenarios"
})

print("Meta Agent Result:", meta_result.content)
```

#### 10.5 Error Handling and Debugging

```lua
-- examples/agents-error-handling.lua
local llmspell = require('llmspell')

-- Enable debug mode for detailed logging
llmspell.config.debug = true
llmspell.config.log_level = "debug"

-- Create agent with error handling
local function create_robust_agent(template_name, tools)
    local success, agent = pcall(function()
        return llmspell.agents.create({
            name = template_name .. "_agent",
            template = template_name,
            tools = tools,
            config = {
                timeout = 30,
                max_retries = 3,
                error_handling = "graceful"
            }
        })
    end)
    
    if not success then
        print("Failed to create agent:", agent)
        return nil
    end
    
    return agent
end

-- Robust agent processing with retry logic
local function robust_process(agent, input, max_attempts)
    max_attempts = max_attempts or 3
    
    for attempt = 1, max_attempts do
        print(string.format("Attempt %d/%d", attempt, max_attempts))
        
        local success, result = pcall(function()
            return agent:process(input)
        end)
        
        if success and result.success then
            return result
        end
        
        if success then
            print("Agent execution failed:", result.error)
        else
            print("Agent call failed:", result)
        end
        
        if attempt < max_attempts then
            print("Retrying in 1 second...")
            os.execute("sleep 1")
        end
    end
    
    return {
        success = false,
        error = "Max retry attempts exceeded"
    }
end

-- Usage example
local research_agent = create_robust_agent("research", {"web_search"})
if research_agent then
    local result = robust_process(research_agent, {
        message = "Research quantum computing developments"
    })
    
    if result.success then
        print("Success:", result.content)
    else
        print("Failed after all retries:", result.error)
    end
end

-- Performance monitoring
local function monitor_agent_performance(agent, input)
    local start_time = os.clock()
    
    local result = agent:process(input)
    
    local end_time = os.clock()
    local duration = end_time - start_time
    
    print(string.format("Agent execution took %.2f seconds", duration))
    
    if result.metadata and result.metadata.performance then
        print("LLM calls:", result.metadata.performance.llm_calls)
        print("Tool calls:", result.metadata.performance.tool_calls)
        print("Memory usage:", result.metadata.performance.memory_mb, "MB")
    end
    
    return result
end
```

### 11. Implementation Checklist

**Week 15 Tasks**:
- [ ] Implement Agent Factory pattern with builders
- [ ] Create Agent Registry with persistence
- [ ] Implement BaseAgent tool integration infrastructure
- [ ] Create tool discovery and registration mechanisms
- [ ] Build agent-as-tool wrapping support
- [ ] Add tool composition patterns
- [ ] Integrate with existing 33+ tool ecosystem
- [ ] Implement script-to-agent integration bridge
- [ ] Register agents with llmspell-bridge
- [ ] Create parameter conversion utilities
- [ ] Implement agent discovery from scripts
- [ ] Build Discovery Service with providers
- [ ] Implement Agent Lifecycle Management
- [ ] Create Agent State Machine
- [ ] Design Agent Templates (Research, Chat, Code, API, Data)

**Week 16 Tasks**:
- [ ] Implement Enhanced ExecutionContext
- [ ] Create Agent Composition patterns
- [ ] Develop example agent configurations
- [ ] Create Lua agent calling examples
- [ ] Update existing Lua examples with agent integration
- [ ] Integration testing with all 41+ tools
- [ ] Performance benchmarking

---

## Testing Strategy

### 1. Unit Tests
- Each tool standardization change
- Security vulnerability fixes
- Agent infrastructure components
- State management operations

### 2. Integration Tests
- Tool chain interactions
- Agent-to-agent communication
- Cross-tool data flow
- Security boundary testing
- Performance regression tests

### 3. Upgrade Validation Tests
- Parameter standardization verification
- Response format consistency checks
- Breaking change documentation accuracy
- Example code validation

### 4. Security Tests
- Penetration testing for each vulnerability
- Resource exhaustion tests
- Sandbox escape attempts
- Permission boundary validation

### 5. Performance Tests
- Tool initialization benchmarks
- Agent lifecycle timing
- Memory usage profiling
- Concurrent execution stress tests

---

## Documentation Requirements

### 1. Breaking Changes Guide
- Complete parameter mapping table
- Response format changes documentation
- Before/after code examples
- Manual upgrade instructions

### 2. Tool Documentation
- Updated schemas for all 41+ tools
- Usage examples with new interfaces
- Performance characteristics
- Security considerations

### 3. Agent Documentation
- Factory pattern explanations
- Registry API reference
- Template usage examples
- Best practices guide

### 4. Security Documentation
- Vulnerability descriptions
- Mitigation strategies
- Security test results
- Ongoing security practices

---

## Risk Mitigation

### 1. Breaking Changes
- **Risk**: Existing scripts break with new interfaces
- **Mitigation**: Clear documentation and example conversions
- **Validation**: Test upgraded example scripts

### 2. Performance Regression
- **Risk**: Security checks slow down tools
- **Mitigation**: Optimize hot paths, use caching
- **Validation**: Continuous benchmarking

### 3. Integration Complexity
- **Risk**: 41+ tools with different patterns
- **Mitigation**: Strict adherence to standards
- **Validation**: Integration test suite

### 4. Security Gaps
- **Risk**: New vulnerabilities introduced
- **Mitigation**: Security review checklist
- **Validation**: Penetration testing

---

## Success Metrics

### Quantitative Metrics
- Tool parameter consistency: 95%
- DRY code compliance: 95%
- Security test coverage: 100%
- Performance maintenance: <10ms initialization
- Documentation completeness: 100%

### Qualitative Metrics
- Developer experience improvement
- Clear documentation and examples
- Clean, maintainable codebase
- Robust agent infrastructure
- Comprehensive security posture

---

## Handoff Criteria

### Phase 3.0 Handoff
- [ ] All 26 tools use standard interfaces
- [ ] Breaking changes documented
- [ ] Security hardening implemented
- [ ] All tests passing

### Phase 3.1 Handoff
- [ ] 16 external tools implemented
- [ ] Following Phase 3.0 standards
- [ ] Integration tests passing
- [ ] Documentation complete

### Phase 3.2 Handoff
- [ ] All security vulnerabilities addressed
- [ ] Performance benchmarks maintained
- [ ] Security test suite complete
- [ ] Resource limits enforced

### Phase 3.3 Handoff
- [ ] All agent infrastructure patterns implemented
- [ ] 41+ tools integrated
- [ ] Example agent configurations functional
- [ ] Full test suite passing

---

This comprehensive design document provides the detailed specifications needed to implement Phase 3's ambitious 8-week plan, transforming the tool library into a standardized, secure, and agent-ready ecosystem.
