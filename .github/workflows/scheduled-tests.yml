name: Scheduled Tests

on:
  schedule:
    # Daily at 2 AM UTC
    - cron: '0 2 * * *'
    # Weekly expensive tests on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      include_expensive:
        description: 'Include expensive tests (webapp-creator)'
        required: false
        default: false
        type: boolean

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Determine which tests to run based on schedule
  determine-tests:
    name: Determine Test Level
    runs-on: ubuntu-latest
    outputs:
      test_level: ${{ steps.set-level.outputs.level }}

    steps:
      - name: Set test level
        id: set-level
        run: |
          # Check if this is the weekly run (Sunday at 3 AM)
          if [ "${{ github.event.schedule }}" == "0 3 * * 0" ]; then
            echo "level=expensive" >> $GITHUB_OUTPUT
            echo "Running weekly expensive tests"
          elif [ "${{ inputs.include_expensive }}" == "true" ]; then
            echo "level=expensive" >> $GITHUB_OUTPUT
            echo "Running expensive tests (manual trigger)"
          else
            echo "level=full" >> $GITHUB_OUTPUT
            echo "Running daily full tests"
          fi

  # Nightly/Weekly test suite
  scheduled-tests:
    name: Scheduled Tests (${{ needs.determine-tests.outputs.test_level }})
    runs-on: ubuntu-latest
    needs: determine-tests
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Cache cargo build
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/
          key: ${{ runner.os }}-cargo-scheduled-${{ hashFiles('**/Cargo.lock') }}

      - name: Run scheduled tests
        env:
          TEST_LEVEL: ${{ needs.determine-tests.outputs.test_level }}
          RUN_EXPENSIVE_TESTS: ${{ needs.determine-tests.outputs.test_level == 'expensive' && '1' || '0' }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'stub-key' }}
        run: |
          echo "Running $TEST_LEVEL tests..."
          ./scripts/ci-test.sh

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: scheduled-test-results-${{ needs.determine-tests.outputs.test_level }}
          path: test-reports/

      - name: Upload application validation report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: validation-report-scheduled
          path: |
            test-reports/*.html
            test-reports/*.json

      # Notify on failure (optional - requires setting up GitHub notifications)
      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const date = new Date().toISOString().split('T')[0];
            const level = '${{ needs.determine-tests.outputs.test_level }}';

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `🔴 Scheduled ${level} tests failed - ${date}`,
              body: `## Scheduled Test Failure

              **Date:** ${date}
              **Test Level:** ${level}
              **Workflow Run:** [View Run](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

              ### Action Items
              - [ ] Review test failures in the workflow run
              - [ ] Check application validation reports
              - [ ] Fix identified issues
              - [ ] Re-run tests manually if needed

              ### Test Reports
              Test reports have been uploaded as artifacts to the workflow run.
              `,
              labels: ['test-failure', 'automated', 'priority-high']
            });

  # Coverage analysis (weekly)
  coverage:
    name: Code Coverage Analysis
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 3 * * 0' || github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/
          key: ${{ runner.os }}-cargo-coverage-${{ hashFiles('**/Cargo.lock') }}

      - name: Install tarpaulin
        run: cargo install cargo-tarpaulin

      - name: Generate coverage report
        run: |
          cargo tarpaulin \
            --workspace \
            --all-features \
            --out Html \
            --output-dir coverage \
            --timeout 300 \
            --skip-clean \
            --exclude-files "*/tests/*" \
            --exclude-files "*/examples/*"

      - name: Upload coverage report
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report
          path: coverage/

      - name: Check coverage threshold
        run: |
          # Extract coverage percentage from tarpaulin output
          COVERAGE=$(cargo tarpaulin --print-summary 2>&1 | grep "Coverage" | grep -oE "[0-9]+\.[0-9]+%" | tr -d '%')

          echo "Code coverage: ${COVERAGE}%"

          # Check against threshold (adjust as needed)
          THRESHOLD=70
          if (( $(echo "$COVERAGE < $THRESHOLD" | bc -l) )); then
            echo "⚠️ Coverage ${COVERAGE}% is below threshold of ${THRESHOLD}%"
            exit 1
          else
            echo "✅ Coverage ${COVERAGE}% meets threshold of ${THRESHOLD}%"
          fi

  # Performance regression detection
  performance:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 2 * * *' || github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/
          key: ${{ runner.os }}-cargo-perf-${{ hashFiles('**/Cargo.lock') }}

      - name: Download previous benchmark results
        uses: actions/download-artifact@v3
        with:
          name: benchmark-baseline
          path: baseline/
        continue-on-error: true

      - name: Run benchmarks
        run: |
          cargo bench --workspace --all-features -- --output-format bencher | tee current-bench.txt

      - name: Compare benchmarks
        run: |
          if [ -f "baseline/benchmark.txt" ]; then
            echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Simple comparison (could be enhanced with proper tooling)
            echo "### Current vs Baseline" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            diff -u baseline/benchmark.txt current-bench.txt || true
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "No baseline found, establishing new baseline"
          fi

      - name: Save benchmark baseline
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-baseline
          path: current-bench.txt

  # Summary report
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    if: always()
    needs: [scheduled-tests, coverage, performance]

    steps:
      - name: Generate summary
        run: |
          echo "# Scheduled Test Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Scheduled Tests: ${{ needs.scheduled-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Coverage Analysis: ${{ needs.coverage.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Performance Check: ${{ needs.performance.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall status
          if [ "${{ needs.scheduled-tests.result }}" == "failure" ] || \
             [ "${{ needs.coverage.result }}" == "failure" ] || \
             [ "${{ needs.performance.result }}" == "failure" ]; then
            echo "## ❌ Overall Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "⚠️ Automated issue has been created for failures." >> $GITHUB_STEP_SUMMARY
          else
            echo "## ✅ Overall Status: PASSED" >> $GITHUB_STEP_SUMMARY
          fi