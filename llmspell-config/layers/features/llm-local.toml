# Local LLM Feature Layer
# Local LLM providers (Ollama, Candle)

[metadata]
name = "llm-local"
category = "feature"
description = "Local LLM (Ollama/Candle)"
use_cases = [
    "Offline LLM inference",
    "Privacy-focused workflows",
    "Zero API costs",
]
features = [
    "TODO: Add features"
]


[profile]
name = "Local LLM Features"
description = "Ollama and Candle local provider support"

# Provider configuration
[providers]
default_provider = "ollama"

[providers.ollama]
name = "ollama"
provider_type = "ollama"
enabled = true
base_url = "http://localhost:11434"
default_model = "llama3.2"
timeout_seconds = 300

[providers.candle]
name = "candle"
provider_type = "candle"
enabled = false  # Requires explicit enablement
default_model = "microsoft/phi-2"
