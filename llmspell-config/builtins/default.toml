# Default Configuration Profile
# Simple local LLM setup using Ollama with sensible defaults
# Suitable for: General purpose scripting, template execution, agent development

default_engine = "lua"

[engines.lua]
stdlib = "All"

[providers]
default_provider = "default"

[providers.default]
provider_type = "ollama"
default_model = "llama3.2:3b"
temperature = 0.7
max_tokens = 4096
timeout_seconds = 30
max_retries = 3

[runtime]
log_level = "info"
