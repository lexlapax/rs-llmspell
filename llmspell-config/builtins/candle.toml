# Candle Configuration Profile
# Embedded LLM inference via Candle backend
# Requires: Models downloaded to ~/.llmspell/models/candle/

default_engine = "lua"

[engines.lua]
stdlib = "All"

[providers.candle]
provider_type = "candle"
enabled = true
timeout_seconds = 300
default_model = "llama3.1:8b-q4"
model_directory = "${HOME}/.llmspell/models/candle"
device = "auto"
max_concurrent = 1
default_quantization = "Q4_K_M"
cpu_threads = 0
context_size = 4096
batch_size = 512
use_flash_attention = true

[runtime]
log_level = "info"
