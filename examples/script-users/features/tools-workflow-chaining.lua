-- Example: Tools - Workflow Chaining
-- Purpose: Multi-tool workflow demonstrations showing how to chain tools together
-- Prerequisites: None (tools work locally)
-- Expected Output: Tool chaining examples with data flow between tools
-- Version: 0.7.0
-- Tags: tools, workflow, chaining

-- ABOUTME: Multi-tool workflow demonstrations
-- ABOUTME: Shows how to chain tools together for complex operations
-- Helper function to execute tool using synchronous API
local function use_tool(tool_name, params)
    local result = Tool.invoke(tool_name, params)
    
    -- Tool.invoke returns structured results directly
    if result then
        return result
    end
    
    -- Return error result if no result
    return {success = false, error = "Tool returned no result"}
end

print("ðŸ”— Multi-Tool Workflow Examples")
print("=================================")
print()

-- Helper function to show workflow steps
local function show_workflow(name, description)
    print(string.format("[1m[35m=== %s ===[0m", name))
    print(description)
    print()
end

-- Workflow 1: Data Processing Pipeline
show_workflow("Data Processing Pipeline", "Fetch data â†’ Process â†’ Validate â†’ Transform")

-- Step 1: Generate test data
local test_id = helpers.execute_tool("uuid_generator", {operation = "generate", format = "standard"})
local uuid_value = nil
if test_id.success and test_id.result and test_id.result.uuid then
    uuid_value = test_id.result.uuid
end
print("Step 1 - Generated test ID:", uuid_value or "failed")

-- Step 2: Create JSON data
local json_data = {
    id = uuid_value or "unknown",
    name = "Test Workflow",
    timestamp = os.date("!%Y-%m-%dT%H:%M:%SZ"),
    values = {10, 20, 30, 40, 50}
}

-- Step 3: Process with JSON tool
-- First convert Lua table to JSON string
local json_string = string.format(
    '{"id":"%s","name":"%s","timestamp":"%s","values":[%s]}',
    json_data.id,
    json_data.name,
    json_data.timestamp,
    table.concat(json_data.values, ",")
)
-- Parse and pretty-print using jq
local json_result = helpers.execute_tool("json_processor", {
    operation = "query",
    input = json_string,
    query = "."  -- Identity query to pretty-print
})
print("\nStep 2 - Formatted JSON:")
if json_result.success then
    -- The result should be in json_result (direct value for JSON processor)
    print("Success - JSON processed")
else
    print("Error:", json_result.error)
end

-- Step 4: Calculate checksum
local checksum = helpers.execute_tool("hash_calculator", {
    operation = "hash",
    algorithm = "sha256",
    input = "test data for checksum"  -- Use simple test data since json_result format varies
})
local hash_value = nil
if checksum.success and checksum.result and checksum.result.hash then
    hash_value = checksum.result.hash
end
print("\nStep 3 - Data checksum:", hash_value or "failed")

-- Step 5: Encode for transmission
local to_encode = json_string
local encoded = helpers.execute_tool("base64_encoder", {
    operation = "encode",
    input = to_encode
})
if encoded.success and encoded.result and encoded.result.encoded then
    local b64_value = encoded.result.encoded
    print("\nStep 4 - Base64 encoded (first 50 chars):", b64_value:sub(1, 50) .. "...")
else
    print("\nStep 4 - Base64 encoding error:", encoded.error)
end

print("\nâœ… Data processing pipeline complete!")

-- Workflow 2: File Analysis Pipeline
show_workflow("File Analysis Workflow", "Create file â†’ Analyze â†’ Transform â†’ Archive")

-- Step 1: Create test file content
local file_content = helpers.execute_tool("template_engine", {
    engine = "handlebars",
    input = [[  -- Already using 'input' parameter
# Analysis Report {{report_id}}
Generated: {{timestamp}}

## Data Summary
Total Items: {{total}}
Average Value: {{average}}

## Items
{{#each items}}
- Item {{@index}}: {{this}}
{{/each}}
]],
    context = {
        report_id = "WF-2025-001",
        timestamp = os.date("!%Y-%m-%d %H:%M:%S UTC"),
        total = 5,
        average = 30,
        items = {10, 20, 30, 40, 50}
    }
})
print("Step 1 - Generated report content")

-- Step 2: Convert to different formats
local uppercase_report = helpers.execute_tool("text_manipulator", {
    operation = "uppercase",
    input = (file_content.success and file_content.result and file_content.result.rendered) or "Sample report content"
})
print("\nStep 2 - Converted to uppercase (first line):")
if uppercase_report.success and uppercase_report.result and uppercase_report.result.output then
    print(uppercase_report.result.output:match("^[^\n]+") or "No first line found")
else
    print("Text manipulation failed")
end

-- Step 3: Calculate diff between versions
local file_text = (file_content.success and file_content.result and file_content.result.rendered) or "Original text"
local upper_text = (uppercase_report.success and uppercase_report.result and uppercase_report.result.output) or "UPPER TEXT"
local diff_result = helpers.execute_tool("diff_calculator", {
    operation = "text_diff",
    old_text = file_text,
    new_text = upper_text,
    format = "unified",
    context_lines = 1
})
if diff_result.success then
    print("\nStep 3 - Generated diff: Success")
else
    print("\nStep 3 - Generated diff: Failed")
end

print("\nâœ… File analysis workflow complete!")

-- Workflow 3: System Monitoring Pipeline
show_workflow("System Monitoring Workflow", "Read env â†’ Check system â†’ Process data â†’ Report")

-- Step 1: Read environment
local env_data = helpers.execute_tool("environment_reader", {
    operation = "get_all",
    filter = "PATH|HOME|USER"
})
print("Step 1 - Read environment variables")

-- Step 2: Get system info
local system_info = helpers.execute_tool("system_monitor", {
    operation = "info"
})
print("\nStep 2 - Retrieved system information")

-- Step 3: Create monitoring report
local memory_percent = helpers.execute_tool("calculator", {
    operation = "evaluate",  -- Added operation parameter
    input = "100 * 0.75"  -- Already using 'input' parameter
})
local calc_result = nil
if memory_percent.success then
    -- Calculator returns direct result as number
    calc_result = tostring(memory_percent)
end
print("\nStep 3 - Calculated metrics:", (calc_result or "N/A") .. "% memory usage")

-- Step 4: Generate timestamp
local timestamp = helpers.execute_tool("date_time_handler", {
    operation = "now",
    format = "%Y-%m-%d %H:%M:%S"
})
local timestamp_value = nil
if timestamp.success and timestamp.result and timestamp.result.datetime then
    timestamp_value = timestamp.result.datetime
end
print("\nStep 4 - Report timestamp:", timestamp_value or "N/A")

print("\nâœ… System monitoring workflow complete!")

-- Workflow 4: Data Validation Pipeline
show_workflow("Data Validation Workflow", "Generate â†’ Validate â†’ Transform â†’ Store")

-- Step 1: Generate test data
local uuid_data = {}
for i = 1, 3 do
    local id = helpers.execute_tool("uuid_generator", {operation = "generate", format = "standard"})
    if id.success and id.result and id.result.uuid then
        table.insert(uuid_data, id.result.uuid)
    end
end
print("Step 1 - Generated", #uuid_data, "UUIDs")

-- Step 2: Create CSV data
local csv_content = "id,name,value\n"
for i, uuid in ipairs(uuid_data) do
    csv_content = csv_content .. string.format("%s,Item-%d,%d\n", uuid, i, i * 100)
end

-- Step 3: Analyze CSV
local csv_analysis = helpers.execute_tool("csv_analyzer", {
    operation = "analyze",
    input = csv_content
})
print("\nStep 2 - CSV analysis complete")

-- Step 4: Transform data
local hash_result = helpers.execute_tool("hash_calculator", {
    operation = "hash",
    algorithm = "md5",
    input = csv_content  -- Already using 'input' parameter
})
local data_hash = nil
if hash_result.success and hash_result.result and hash_result.result.hash then
    data_hash = hash_result.result.hash
end

local transform_data = {
    source = "workflow",
    timestamp = os.date("!%Y-%m-%dT%H:%M:%SZ"),
    records = #uuid_data,
    data_hash = data_hash or "error"
}

-- Step 5: Format final output
-- Convert Lua table to JSON string for processing
local meta_json = string.format(
    '{"source":"%s","timestamp":"%s","records":%d,"data_hash":"%s"}',
    transform_data.source,
    transform_data.timestamp,
    transform_data.records,
    transform_data.data_hash
)
local final_json = helpers.execute_tool("json_processor", {
    operation = "query",
    input = meta_json,
    query = "."  -- Pretty print
})
print("\nStep 3 - Final metadata:")
if final_json.success then
    print("JSON processed successfully")
else
    print("Error:", final_json.error)
end

print("\nâœ… Data validation workflow complete!")

-- Workflow 5: Cross-Tool Error Handling
show_workflow("Error Handling Workflow", "Demonstrate error propagation and recovery")

-- Test invalid operations
print("Testing error handling across tools:")

-- Invalid hash algorithm
local hash_error = helpers.execute_tool("hash_calculator", {
    operation = "hash",
    algorithm = "invalid-algo",
    input = "test"  -- Already using 'input' parameter
})
print("\n1. Invalid hash algorithm:", hash_error.success and "Success" or "Failed")
if hash_error.error then
    print("   Error:", hash_error.error)
end

-- Invalid JSON
local json_error = helpers.execute_tool("json_processor", {
    operation = "validate",
    input_json = "{invalid json"
})
print("\n2. Invalid JSON parsing:", json_error.success and "Success" or "Failed")
if json_error.error then
    print("   Error:", json_error.error:match("^[^:]+"))
end

-- Recovery: Use fallback
if not json_error.success then
    local fallback = helpers.execute_tool("json_processor", {
        operation = "format",
        input_json = {error = "Invalid input", fallback = true},
        pretty = true
    })
    print("\n3. Fallback recovery:", fallback.success and "Success" or "Failed")
end

print("\nâœ… Error handling workflow complete!")

-- Summary
print("\n" .. string.rep("=", 50))
print("ðŸ“Š Workflow Examples Summary")
print(string.rep("=", 50))
print()
print("Demonstrated workflows:")
print("  1. Data Processing Pipeline - Chain data transformation")
print("  2. File Analysis Workflow - Content generation and analysis")
print("  3. System Monitoring - Environment and system data collection")
print("  4. Data Validation - Generate, validate, and transform data")
print("  5. Error Handling - Graceful error recovery across tools")
print()
print("Key concepts shown:")
print("  - Tool output chaining")
print("  - Error propagation and recovery")
print("  - Data transformation pipelines")
print("  - Cross-tool integration patterns")

-- Return summary for test runner
return {
    status = "success",
    workflows = 5,
    concepts = {
        "output_chaining",
        "error_handling",
        "data_pipelines",
        "tool_integration"
    }
}