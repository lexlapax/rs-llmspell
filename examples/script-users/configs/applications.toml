# Configuration for Real-World Applications
# This file configures all 7 production applications with real LLM providers

[general]
name = "Production Applications Config"
version = "0.7.0"
environment = "development"  # development | staging | production

# API Provider Configuration
# CRITICAL: All applications require real API keys - NO MOCKS ALLOWED
[providers]

  [providers.openai]
  enabled = true
  # API key from environment: OPENAI_API_KEY
  default_model = "gpt-4o-mini"  # Use cheaper model for development
  max_tokens = 2000
  temperature = 0.7
  rate_limit = 60  # requests per minute
  retry_attempts = 3
  retry_delay = 1000  # milliseconds

  [providers.anthropic]
  enabled = true
  # API key from environment: ANTHROPIC_API_KEY
  default_model = "claude-3-haiku-20240307"  # Use cheaper model for development
  max_tokens = 2000
  temperature = 0.7
  rate_limit = 50  # requests per minute
  retry_attempts = 3
  retry_delay = 1000  # milliseconds

# Application-Specific Configurations

[applications.ai_research_assistant]
enabled = true
agents = [
  { name = "research_orchestrator", provider = "openai", model = "gpt-4o-mini", temperature = 0.3 },
  { name = "knowledge_synthesis", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.5 },
  { name = "output_generation", provider = "openai", model = "gpt-4o-mini", temperature = 0.7 }
]
max_research_time = 300  # seconds
max_sources = 10
enable_caching = true

[applications.data_pipeline]
enabled = true
agents = [
  { name = "data_quality_analyzer", provider = "openai", model = "gpt-4o-mini", temperature = 0.3 },
  { name = "anomaly_detector", provider = "openai", model = "gpt-4o-mini", temperature = 0.4 },
  { name = "report_generator", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.6 }
]
batch_size = 100
checkpoint_interval = 100
enable_monitoring = true

[applications.monitoring_system]
enabled = true
agents = [
  { name = "log_analyzer", provider = "openai", model = "gpt-4o-mini", temperature = 0.2 },
  { name = "predictive_analyzer", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.4 },
  { name = "incident_reporter", provider = "openai", model = "gpt-4o-mini", temperature = 0.5 },
  { name = "remediation_suggester", provider = "openai", model = "gpt-4o-mini", temperature = 0.6 }
]
analysis_interval = 60  # seconds
alert_cooldown = 300  # seconds
dashboard_refresh = 30  # seconds

[applications.customer_support_bot]
enabled = true
agents = [
  { name = "conversation_agent", provider = "openai", model = "gpt-4o-mini", temperature = 0.7 },
  { name = "knowledge_base_agent", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.3 },
  { name = "escalation_agent", provider = "openai", model = "gpt-4o-mini", temperature = 0.5 },
  { name = "sentiment_analyzer", provider = "openai", model = "gpt-4o-mini", temperature = 0.4 }
]
max_conversation_history = 20
escalation_threshold = 0.3  # sentiment score
response_timeout = 30  # seconds

[applications.content_generation_system]
enabled = true
agents = [
  { name = "content_creator", provider = "openai", model = "gpt-4o-mini", temperature = 0.8 },
  { name = "quality_checker", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.3 },
  { name = "seo_optimizer", provider = "openai", model = "gpt-4o-mini", temperature = 0.5 },
  { name = "localizer", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.6 }
]
batch_size = 50
quality_threshold = 0.8
enable_plagiarism_check = true

[applications.code_review_assistant]
enabled = true
# Also requires GITHUB_TOKEN in environment
agents = [
  { name = "code_analyzer", provider = "openai", model = "gpt-4o-mini", temperature = 0.2 },
  { name = "security_reviewer", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.1 },
  { name = "performance_analyzer", provider = "openai", model = "gpt-4o-mini", temperature = 0.3 },
  { name = "best_practices_checker", provider = "openai", model = "gpt-4o-mini", temperature = 0.4 }
]
max_diff_size = 5000  # lines
security_scan_enabled = true
auto_fix_suggestions = true

[applications.web_app_generator]
enabled = true
agents = [
  { name = "requirements_analyzer", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.3 },
  { name = "architecture_designer", provider = "openai", model = "gpt-4o-mini", temperature = 0.4 },
  { name = "frontend_generator", provider = "openai", model = "gpt-4o-mini", temperature = 0.6 },
  { name = "backend_generator", provider = "anthropic", model = "claude-3-haiku-20240307", temperature = 0.5 },
  { name = "test_generator", provider = "openai", model = "gpt-4o-mini", temperature = 0.4 }
]
supported_frameworks = ["react", "vue", "express", "fastapi"]
generate_tests = true
generate_docs = true

# Cost Management
[cost_management]
enabled = true
daily_limit_usd = 10.00  # Daily spending limit
alert_threshold_usd = 5.00  # Alert when approaching limit
track_per_application = true
track_per_agent = true

# For production, use stronger models:
# - OpenAI: gpt-4-turbo, gpt-4o
# - Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229

# Testing Configuration
[testing]
use_cheaper_models = true  # Override to use mini/haiku models
max_test_tokens = 100  # Limit tokens during tests
mock_external_apis = true  # Mock non-LLM APIs (like GitHub)
enable_cost_tracking = true  # Track costs during test runs