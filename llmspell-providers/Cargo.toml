[package]
name = "llmspell-providers"
description = "LLM provider integrations for rs-llmspell"
version.workspace = true
edition.workspace = true
authors.workspace = true
repository.workspace = true
license.workspace = true

[dependencies]
llmspell-core = { path = "../llmspell-core" }
llmspell-utils = { path = "../llmspell-utils" }
tokio.workspace = true
async-trait.workspace = true
serde.workspace = true
serde_json.workspace = true
anyhow.workspace = true
tracing.workspace = true
rig-core.workspace = true
reqwest = { version = "0.12", default-features = false }  # For rig-core 0.23 Ollama type parameter
ollama-rs = "0.3.2"
url = "2"
chrono.workspace = true
candle-nn.workspace = true
candle-transformers.workspace = true
hf-hub.workspace = true
tokenizers.workspace = true
parking_lot.workspace = true
rand.workspace = true
dirs = "5.0"
# Phase 13c.1.7: ureq kept (used in hf_downloader.rs for synchronous HuggingFace model downloads)
# Reason: Candle model loading is sync, reqwest is async - using ureq avoids blocking executor
ureq = "2.12"

# Platform-specific dependencies for GPU support
[target.'cfg(target_os = "macos")'.dependencies]
candle-core = { workspace = true, features = ["metal"] }

[target.'cfg(not(target_os = "macos"))'.dependencies]
candle-core = { workspace = true }

[dev-dependencies]
tempfile.workspace = true